# Task 3.7 Fix 07-F: æœ€é©åŒ–ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼çµ±åˆ

## æ¦‚è¦
å…¨ã¦ã®æœ€é©åŒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ±åˆã—ãŸOptimizedStreamProcessorã‚’å®Ÿè£…ã—ã€æ—¢å­˜ã®streamãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ®µéšçš„ã«ç½®ãæ›ãˆã¾ã™ã€‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã€ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦ã€ç›®æ¨™ã¨ã™ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’é”æˆã—ã¾ã™ã€‚

## å„ªå…ˆåº¦
**ğŸ”´ æœ€é«˜å„ªå…ˆåº¦** - å…¨æœ€é©åŒ–ã®çµ±åˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™é”æˆ

## å®Ÿè£…æ™‚é–“è¦‹ç©ã‚‚ã‚Š
**90åˆ†** - é›†ä¸­ä½œæ¥­æ™‚é–“

## ä¾å­˜é–¢ä¿‚
- Task 3.7 Fix 07-Aã€œE å…¨ã¦å®Œäº†å¿…é ˆ

## å—ã‘å…¥ã‚ŒåŸºæº–

### çµ±åˆè¦ä»¶
- [ ] å…¨æœ€é©åŒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆå®Œäº†
- [ ] æ—¢å­˜streamãƒ¡ã‚½ãƒƒãƒ‰ã®æ®µéšçš„ç½®ãæ›ãˆ
- [ ] å¾Œæ–¹äº’æ›æ€§ã®ç¶­æŒ
- [ ] æ—¢å­˜ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®100%ãƒ‘ã‚¹

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶
- [ ] ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: 1000 â†’ 2000 req/s é”æˆ
- [ ] ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼: P95 50ms â†’ 25msä»¥ä¸‹
- [ ] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 30%å‰Šæ¸›
- [ ] åŒæ™‚æ¥ç¶šæ•°: 100æ¥ç¶šå®‰å®šå‡¦ç†

### å®‰å®šæ€§è¦ä»¶
- [ ] é«˜è² è·çŠ¶æ…‹ã§ã®ã‚·ã‚¹ãƒ†ãƒ å®‰å®šæ€§
- [ ] ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ãƒ‡ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
- [ ] ã‚¨ãƒ©ãƒ¼ç‡1%ä»¥ä¸‹ç¶­æŒ
- [ ] ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯å®Œå…¨é˜²æ­¢

### ç›£è¦–ãƒ»ãƒ‡ãƒãƒƒã‚°è¦ä»¶
- [ ] çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
- [ ] è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
- [ ] æœ€é©åŒ–åŠ¹æœã®å®šé‡çš„æ¸¬å®š
- [ ] ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ©Ÿèƒ½

## æŠ€è¡“çš„è©³ç´°

### OptimizedStreamProcessor çµ±åˆå®Ÿè£…

#### src/grpc/performance/processor.rs
```rust
//! æœ€é©åŒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼çµ±åˆ
//! 
//! å…¨ã¦ã®æœ€é©åŒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ±åˆã—ã€Unity MCP Server ã®
//! ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã«ãŠã„ã¦æœ€å¤§é™ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã‚’å®Ÿç¾ã™ã‚‹ã€‚

use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use std::collections::HashMap;
use tokio::sync::{mpsc, Semaphore};
use tokio_stream::{wrappers::ReceiverStream, StreamExt};
use tonic::{Request, Response, Status, Streaming};
use uuid::Uuid;
use tracing::{debug, info, warn, error, instrument};

use crate::unity::{StreamRequest, StreamResponse, stream_response};
use crate::grpc::service::UnityMcpServiceImpl;
use crate::grpc::performance::{
    OptimizationConfig, OptimizationContext, OptimizationResult, OptimizationError,
    monitor::StreamPerformanceMonitor,
    resource_pool::ResourcePool,
    cache::StreamCache,
    worker_pool::{WorkerPool, TaskPriority, TaskContext},
};

/// æœ€é©åŒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼
pub struct OptimizedStreamProcessor {
    // ã‚³ã‚¢æœ€é©åŒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
    performance_monitor: Arc<StreamPerformanceMonitor>,
    resource_pool: Arc<ResourcePool>,
    cache_system: Arc<StreamCache>,
    worker_pool: Arc<WorkerPool>,
    
    // è¨­å®šã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    config: OptimizationConfig,
    
    // ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡
    connection_limiter: Arc<Semaphore>,
    backpressure_controller: Arc<BackpressureController>,
    
    // çµ±è¨ˆã¨ç›£è¦–
    processor_stats: Arc<Mutex<ProcessorStatistics>>,
    optimization_metrics: Arc<Mutex<OptimizationMetrics>>,
    
    // ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ•ãƒ©ã‚°
    feature_flags: FeatureFlags,
}

/// ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼çµ±è¨ˆ
#[derive(Debug, Default, Clone)]
pub struct ProcessorStatistics {
    pub total_connections: u64,
    pub active_connections: u64,
    pub total_messages_processed: u64,
    pub cache_hit_rate: f64,
    pub worker_pool_utilization: f64,
    pub avg_end_to_end_latency: Duration,
    pub backpressure_events: u64,
    pub optimization_effectiveness: f64,
}

/// æœ€é©åŒ–ãƒ¡ãƒˆãƒªã‚¯ã‚¹
#[derive(Debug, Default, Clone)]
pub struct OptimizationMetrics {
    pub throughput_improvement: f64,      // æ”¹å–„å€ç‡
    pub latency_reduction: f64,           // å‰Šæ¸›ç‡
    pub memory_efficiency_gain: f64,      // åŠ¹ç‡åŒ–ç‡
    pub cpu_utilization_optimization: f64,
    pub overall_performance_score: f64,   // ç·åˆã‚¹ã‚³ã‚¢
}

/// ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡
#[derive(Debug)]
pub struct BackpressureController {
    current_load: Arc<Mutex<f64>>,
    thresholds: BackpressureThresholds,
    adaptive_settings: Arc<Mutex<AdaptiveBackpressureSettings>>,
}

/// ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼é–¾å€¤
#[derive(Debug, Clone)]
pub struct BackpressureThresholds {
    pub warning_threshold: f64,    // 0.7
    pub critical_threshold: f64,   // 0.8
    pub emergency_threshold: f64,  // 0.9
}

/// é©å¿œçš„ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼è¨­å®š
#[derive(Debug, Clone)]
pub struct AdaptiveBackpressureSettings {
    pub current_limit: f64,
    pub adjustment_factor: f64,
    pub last_adjustment: Instant,
}

/// ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ•ãƒ©ã‚°
#[derive(Debug, Clone)]
pub struct FeatureFlags {
    pub enable_caching: bool,
    pub enable_batching: bool,
    pub enable_compression: bool,
    pub enable_adaptive_optimization: bool,
    pub enable_detailed_metrics: bool,
}

impl Default for FeatureFlags {
    fn default() -> Self {
        Self {
            enable_caching: true,
            enable_batching: true,
            enable_compression: true,
            enable_adaptive_optimization: true,
            enable_detailed_metrics: false, // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é‡è¦–
        }
    }
}

/// å‡¦ç†ã‚¿ã‚¹ã‚¯ï¼ˆçµ±åˆç‰ˆï¼‰
#[derive(Debug)]
pub struct OptimizedProcessingTask {
    pub task_id: Uuid,
    pub connection_id: String,
    pub message_id: u64,
    pub request: StreamRequest,
    pub submitted_at: Instant,
    pub optimization_context: OptimizationContext,
    pub priority: TaskPriority,
}

impl OptimizedStreamProcessor {
    /// æ–°ã—ã„æœ€é©åŒ–ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’ä½œæˆ
    pub fn new(config: OptimizationConfig) -> Self {
        info!("Initializing optimized stream processor");

        // ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        let performance_monitor = Arc::new(StreamPerformanceMonitor::new());
        let resource_pool = Arc::new(ResourcePool::with_config(
            config.to_resource_pool_config()
        ));
        let cache_system = Arc::new(StreamCache::with_config(
            config.to_cache_config()
        ));
        let worker_pool = Arc::new(
            WorkerPool::with_config(config.to_worker_pool_config())
                .with_performance_monitor(Arc::clone(&performance_monitor))
        );

        // ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡
        let connection_limiter = Arc::new(Semaphore::new(config.max_concurrent_connections));
        let backpressure_controller = Arc::new(BackpressureController::new(
            BackpressureThresholds {
                warning_threshold: config.backpressure_threshold * 0.8,
                critical_threshold: config.backpressure_threshold,
                emergency_threshold: config.backpressure_threshold * 1.2,
            }
        ));

        let processor = Self {
            performance_monitor,
            resource_pool,
            cache_system,
            worker_pool,
            config: config.clone(),
            connection_limiter,
            backpressure_controller,
            processor_stats: Arc::new(Mutex::new(ProcessorStatistics::default())),
            optimization_metrics: Arc::new(Mutex::new(OptimizationMetrics::default())),
            feature_flags: FeatureFlags::default(),
        };

        // çµ±è¨ˆæ›´æ–°ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
        processor.start_statistics_update_tasks();

        info!("Optimized stream processor initialized successfully");
        processor
    }

    /// æœ€é©åŒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
    #[instrument(skip(self, incoming_stream))]
    pub async fn process_stream_optimized(
        &self,
        mut incoming_stream: Streaming<StreamRequest>,
        response_sender: mpsc::Sender<Result<StreamResponse, Status>>,
    ) {
        let connection_id = Uuid::new_v4().to_string();
        let session_start = Instant::now();
        
        // æ¥ç¶šåˆ¶é™ãƒã‚§ãƒƒã‚¯
        let _connection_permit = match self.connection_limiter.acquire().await {
            Ok(permit) => permit,
            Err(_) => {
                error!(connection_id = %connection_id, "Connection limit exceeded");
                let _ = response_sender.send(Err(Status::resource_exhausted("Too many connections"))).await;
                return;
            }
        };

        info!(connection_id = %connection_id, "Starting optimized stream processing");

        // æœ€é©åŒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆæœŸåŒ–
        let mut optimization_context = OptimizationContext::new(
            connection_id.clone(),
            self.config.clone()
        );

        // ãƒãƒƒãƒãƒ³ã‚°ç”¨ãƒãƒƒãƒ•ã‚¡
        let mut batch_buffer = Vec::with_capacity(self.config.batch_size);
        let mut message_counter = 0u64;
        let mut last_batch_flush = Instant::now();

        // çµ±è¨ˆæ›´æ–°
        self.increment_active_connections().await;

        // ãƒ¡ã‚¤ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ«ãƒ¼ãƒ—
        while let Some(result) = incoming_stream.next().await {
            match result {
                Ok(stream_request) => {
                    message_counter += 1;
                    optimization_context.increment_message_count();

                    // ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ãƒã‚§ãƒƒã‚¯
                    if self.should_apply_backpressure().await {
                        self.handle_backpressure(&response_sender, &connection_id).await;
                        continue;
                    }

                    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
                    if self.feature_flags.enable_caching {
                        if let Some(cached_response) = self.cache_system.get(&stream_request).await {
                            debug!(
                                connection_id = %connection_id,
                                message_id = message_counter,
                                "Cache hit - returning cached response"
                            );
                            
                            if let Err(_) = response_sender.send(Ok(cached_response)).await {
                                warn!("Response channel closed - client disconnected");
                                break;
                            }
                            continue;
                        }
                    }

                    // ãƒãƒƒãƒå‡¦ç†ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
                    if self.feature_flags.enable_batching && self.should_batch_request(&stream_request) {
                        let task = OptimizedProcessingTask {
                            task_id: Uuid::new_v4(),
                            connection_id: connection_id.clone(),
                            message_id: message_counter,
                            request: stream_request,
                            submitted_at: Instant::now(),
                            optimization_context: optimization_context.clone(),
                            priority: self.determine_task_priority(&stream_request),
                        };

                        batch_buffer.push(task);

                        // ãƒãƒƒãƒã‚µã‚¤ã‚ºã¾ãŸã¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
                        let should_flush = batch_buffer.len() >= self.config.batch_size ||
                                          last_batch_flush.elapsed() >= self.config.max_batch_wait;

                        if should_flush {
                            self.process_batch_optimized(
                                std::mem::take(&mut batch_buffer),
                                &response_sender
                            ).await;
                            last_batch_flush = Instant::now();
                        }
                    } else {
                        // ç›´æ¥å‡¦ç†
                        self.process_single_request_optimized(
                            stream_request,
                            message_counter,
                            &connection_id,
                            &optimization_context,
                            &response_sender,
                        ).await;
                    }
                }
                Err(status) => {
                    warn!(
                        connection_id = %connection_id,
                        error = %status,
                        "Stream error encountered"
                    );

                    // æ®‹ã‚Šãƒãƒƒãƒã‚’å‡¦ç†ã—ã¦ã‹ã‚‰ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹
                    if !batch_buffer.is_empty() {
                        self.process_batch_optimized(batch_buffer, &response_sender).await;
                    }

                    let error_response = self.create_stream_error_response(status);
                    let _ = response_sender.send(Ok(error_response)).await;
                    break;
                }
            }
        }

        // æ®‹ã‚Šãƒãƒƒãƒ•ã‚¡ã‚’å‡¦ç†
        if !batch_buffer.is_empty() {
            self.process_batch_optimized(batch_buffer, &response_sender).await;
        }

        // ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†å‡¦ç†
        let session_duration = session_start.elapsed();
        self.performance_monitor.record_stream_session(
            connection_id.clone(),
            message_counter,
            session_duration,
            optimization_context.calculate_total_bytes_processed(),
        );

        self.decrement_active_connections().await;

        info!(
            connection_id = %connection_id,
            message_count = message_counter,
            duration = ?session_duration,
            "Optimized stream processing completed"
        );
    }

    /// å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æœ€é©åŒ–å‡¦ç†
    #[instrument(skip(self, request, response_sender))]
    async fn process_single_request_optimized(
        &self,
        request: StreamRequest,
        message_id: u64,
        connection_id: &str,
        optimization_context: &OptimizationContext,
        response_sender: &mpsc::Sender<Result<StreamResponse, Status>>,
    ) {
        let processing_start = Instant::now();
        
        // ã‚¿ã‚¹ã‚¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ
        let task_context = TaskContext {
            connection_id: connection_id.to_string(),
            message_id,
            batch_id: None,
            deadline: Some(processing_start + self.config.stream_timeout),
        };

        // ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«ã§å‡¦ç†
        let priority = self.determine_task_priority(&request);
        match self.worker_pool.execute(request.clone(), priority, task_context).await {
            Ok(response) => {
                // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰
                if self.feature_flags.enable_caching {
                    self.cache_system.put(&request, response.clone()).await;
                }

                if let Err(_) = response_sender.send(Ok(response)).await {
                    warn!("Response channel closed during single request processing");
                }
            }
            Err(error) => {
                error!(
                    connection_id = connection_id,
                    message_id = message_id,
                    error = %error,
                    "Single request processing failed"
                );

                let error_response = self.create_processing_error_response(error);
                let _ = response_sender.send(Ok(error_response)).await;
            }
        }

        // å‡¦ç†æ™‚é–“è¨˜éŒ²
        let processing_time = processing_start.elapsed();
        self.performance_monitor.record_single_request_processing(processing_time);

        debug!(
            connection_id = connection_id,
            message_id = message_id,
            processing_time = ?processing_time,
            "Single request processing completed"
        );
    }

    /// ãƒãƒƒãƒã®æœ€é©åŒ–å‡¦ç†
    #[instrument(skip(self, batch, response_sender))]
    async fn process_batch_optimized(
        &self,
        batch: Vec<OptimizedProcessingTask>,
        response_sender: &mpsc::Sender<Result<StreamResponse, Status>>,
    ) {
        if batch.is_empty() {
            return;
        }

        let batch_id = Uuid::new_v4();
        let batch_size = batch.len();
        let batch_start = Instant::now();

        debug!(
            batch_id = %batch_id,
            batch_size = batch_size,
            "Processing optimized batch"
        );

        // ãƒãƒƒãƒã‚’ä¸¦åˆ—å‡¦ç†ç”¨ã®ã‚¿ã‚¹ã‚¯ã«å¤‰æ›
        let processing_tasks: Vec<_> = batch.into_iter()
            .map(|task| {
                let task_context = TaskContext {
                    connection_id: task.connection_id,
                    message_id: task.message_id,
                    batch_id: Some(batch_id),
                    deadline: Some(task.submitted_at + self.config.stream_timeout),
                };
                (task.request, task.priority, task_context)
            })
            .collect();

        // ä¸¦åˆ—å®Ÿè¡Œ
        let mut futures = Vec::new();
        for (request, priority, context) in processing_tasks {
            let worker_pool = Arc::clone(&self.worker_pool);
            let future = async move {
                worker_pool.execute(request, priority, context).await
            };
            futures.push(future);
        }

        // å…¨ã¦ã®ã‚¿ã‚¹ã‚¯å®Œäº†ã‚’å¾…æ©Ÿ
        let results = futures::future::join_all(futures).await;

        // çµæœã‚’ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒãƒ£ãƒãƒ«ã«é€ä¿¡
        for result in results {
            match result {
                Ok(response) => {
                    if let Err(_) = response_sender.send(Ok(response)).await {
                        warn!("Response channel closed during batch processing");
                        break;
                    }
                }
                Err(error) => {
                    error!(error = %error, "Batch task processing failed");
                    let error_response = self.create_processing_error_response(error);
                    let _ = response_sender.send(Ok(error_response)).await;
                }
            }
        }

        let batch_duration = batch_start.elapsed();
        self.performance_monitor.record_batch_processing(batch_size, batch_duration);

        debug!(
            batch_id = %batch_id,
            batch_size = batch_size,
            duration = ?batch_duration,
            throughput = batch_size as f64 / batch_duration.as_secs_f64(),
            "Optimized batch processing completed"
        );
    }

    /// æœ€é©åŒ–çµæœã®å–å¾—
    pub fn get_optimization_result(&self) -> OptimizationResult {
        let performance_summary = self.performance_monitor.get_performance_summary();
        let processor_stats = self.get_processor_statistics();
        
        OptimizationResult {
            throughput: performance_summary.throughput_rps,
            avg_latency: Duration::from_secs_f64(performance_summary.avg_latency_ms / 1000.0),
            p95_latency: Duration::from_secs_f64(performance_summary.p95_latency_ms / 1000.0),
            p99_latency: Duration::from_secs_f64(performance_summary.p99_latency_ms / 1000.0),
            memory_usage: performance_summary.memory_usage_mb as usize * 1024 * 1024,
            cache_hit_ratio: performance_summary.cache_efficiency,
            worker_utilization: performance_summary.worker_utilization,
        }
    }

    /// ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼çµ±è¨ˆã‚’å–å¾—
    pub fn get_processor_statistics(&self) -> ProcessorStatistics {
        let mut stats = self.processor_stats.lock()
            .map(|s| s.clone())
            .unwrap_or_default();

        // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆã‚’æ›´æ–°
        stats.worker_pool_utilization = self.worker_pool.usage_ratio().await;
        stats.cache_hit_rate = self.cache_system.get_statistics().hit_ratio;
        
        stats
    }

    /// æœ€é©åŒ–åŠ¹æœã‚’æ¸¬å®š
    pub async fn measure_optimization_effectiveness(&self) -> OptimizationMetrics {
        // ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼ˆå®Ÿè£…çœç•¥ï¼‰
        let baseline_metrics = self.get_baseline_metrics().await;
        let current_metrics = self.get_optimization_result();

        let throughput_improvement = current_metrics.throughput / baseline_metrics.throughput;
        let latency_reduction = 1.0 - (current_metrics.avg_latency.as_secs_f64() / baseline_metrics.avg_latency.as_secs_f64());
        let memory_efficiency = baseline_metrics.memory_usage as f64 / current_metrics.memory_usage as f64;

        OptimizationMetrics {
            throughput_improvement,
            latency_reduction,
            memory_efficiency_gain: memory_efficiency - 1.0,
            cpu_utilization_optimization: 0.2, // ä»®ã®å€¤
            overall_performance_score: (throughput_improvement + latency_reduction + memory_efficiency) / 3.0,
        }
    }

    // å†…éƒ¨ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰

    async fn should_apply_backpressure(&self) -> bool {
        let current_load = self.backpressure_controller.get_current_load().await;
        current_load > self.backpressure_controller.thresholds.critical_threshold
    }

    async fn handle_backpressure(
        &self,
        response_sender: &mpsc::Sender<Result<StreamResponse, Status>>,
        connection_id: &str,
    ) {
        warn!(connection_id = connection_id, "Applying backpressure due to high load");

        // ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼è­¦å‘Šã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é€ä¿¡
        let backpressure_response = self.create_backpressure_warning();
        let _ = response_sender.send(Ok(backpressure_response)).await;

        // çŸ­æ™‚é–“å¾…æ©Ÿ
        tokio::time::sleep(Duration::from_millis(10)).await;

        self.performance_monitor.record_backpressure_event();
    }

    fn should_batch_request(&self, _request: &StreamRequest) -> bool {
        // ãƒãƒƒãƒå‡¦ç†åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
        true // ç°¡ç•¥åŒ–
    }

    fn determine_task_priority(&self, _request: &StreamRequest) -> TaskPriority {
        // å„ªå…ˆåº¦æ±ºå®šãƒ­ã‚¸ãƒƒã‚¯
        TaskPriority::Normal // ç°¡ç•¥åŒ–
    }

    fn create_stream_error_response(&self, _status: Status) -> StreamResponse {
        // ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆ
        StreamResponse::default() // ç°¡ç•¥åŒ–
    }

    fn create_processing_error_response(&self, _error: crate::grpc::performance::worker_pool::ProcessingError) -> StreamResponse {
        // å‡¦ç†ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆ
        StreamResponse::default() // ç°¡ç•¥åŒ–
    }

    fn create_backpressure_warning(&self) -> StreamResponse {
        // ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼è­¦å‘Šãƒ¬ã‚¹ãƒãƒ³ã‚¹
        StreamResponse::default() // ç°¡ç•¥åŒ–
    }

    async fn increment_active_connections(&self) {
        if let Ok(mut stats) = self.processor_stats.lock() {
            stats.active_connections += 1;
            stats.total_connections += 1;
        }
    }

    async fn decrement_active_connections(&self) {
        if let Ok(mut stats) = self.processor_stats.lock() {
            stats.active_connections = stats.active_connections.saturating_sub(1);
        }
    }

    async fn get_baseline_metrics(&self) -> OptimizationResult {
        // ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šï¼ˆå®Ÿè£…çœç•¥ï¼‰
        OptimizationResult {
            throughput: 1000.0,
            avg_latency: Duration::from_millis(50),
            p95_latency: Duration::from_millis(100),
            p99_latency: Duration::from_millis(200),
            memory_usage: 100 * 1024 * 1024, // 100MB
            cache_hit_ratio: 0.0,
            worker_utilization: 0.3,
        }
    }

    fn start_statistics_update_tasks(&self) {
        // çµ±è¨ˆæ›´æ–°ã‚¿ã‚¹ã‚¯é–‹å§‹
    }
}

impl BackpressureController {
    fn new(thresholds: BackpressureThresholds) -> Self {
        Self {
            current_load: Arc::new(Mutex::new(0.0)),
            thresholds,
            adaptive_settings: Arc::new(Mutex::new(AdaptiveBackpressureSettings {
                current_limit: 1.0,
                adjustment_factor: 0.1,
                last_adjustment: Instant::now(),
            })),
        }
    }

    async fn get_current_load(&self) -> f64 {
        self.current_load.lock()
            .map(|load| *load)
            .unwrap_or(0.0)
    }
}

// è¨­å®šå¤‰æ›ãƒˆãƒ¬ã‚¤ãƒˆå®Ÿè£…
impl OptimizationConfig {
    pub fn to_resource_pool_config(&self) -> crate::grpc::performance::resource_pool::ResourcePoolConfig {
        // å®Ÿè£…çœç•¥
        crate::grpc::performance::resource_pool::ResourcePoolConfig::default()
    }

    pub fn to_cache_config(&self) -> crate::grpc::performance::cache::CacheConfig {
        // å®Ÿè£…çœç•¥  
        crate::grpc::performance::cache::CacheConfig::default()
    }

    pub fn to_worker_pool_config(&self) -> crate::grpc::performance::worker_pool::WorkerPoolConfig {
        // å®Ÿè£…çœç•¥
        crate::grpc::performance::worker_pool::WorkerPoolConfig::default()
    }
}
```

### æ—¢å­˜service.rsã¨ã®çµ±åˆ

#### æ®µéšçš„ç½®ãæ›ãˆå®Ÿè£…
```rust
// src/grpc/service.rs ã¸ã®çµ±åˆ

use crate::grpc::performance::{
    OptimizedStreamProcessor,
    OptimizationConfig,
};

impl UnityMcpService for UnityMcpServiceImpl {
    #[instrument(skip(self))]
    async fn stream(
        &self,
        request: Request<Streaming<StreamRequest>>,
    ) -> Result<Response<Self::StreamStream>, Status> {
        info!("Stream connection established with optimized processor");

        // æœ€é©åŒ–ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’ä½œæˆ
        let optimization_config = OptimizationConfig::default();
        let processor = OptimizedStreamProcessor::new(optimization_config);
        
        let incoming_stream = request.into_inner();
        let (response_sender, response_receiver) = tokio::sync::mpsc::channel(10000);
        
        // æœ€é©åŒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹
        tokio::spawn(async move {
            processor.process_stream_optimized(incoming_stream, response_sender).await;
        });

        // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ä½œæˆ
        let response_stream = tokio_stream::wrappers::ReceiverStream::new(response_receiver);
        let boxed_stream: Self::StreamStream = Box::pin(response_stream);

        Ok(Response::new(boxed_stream))
    }
}
```

## å®Ÿè£…è¨ˆç”»

### Step 1: OptimizedStreamProcessor åŸºç›¤ (30åˆ†)
1. åŸºæœ¬æ§‹é€ ã¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆçµ±åˆ
2. æœ€é©åŒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†
3. ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ•ãƒ©ã‚°å®Ÿè£…

### Step 2: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†çµ±åˆ (35åˆ†)
1. process_stream_optimized ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯
2. ãƒãƒƒãƒå‡¦ç†ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±åˆ
3. ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡

### Step 3: æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ (25åˆ†)
1. service.rs ã®æ®µéšçš„ç½®ãæ›ãˆ
2. å¾Œæ–¹äº’æ›æ€§ç¢ºä¿
3. çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

## ãƒ†ã‚¹ãƒˆè¦ä»¶

### çµ±åˆãƒ†ã‚¹ãƒˆ
```rust
#[cfg(test)]
mod integration_tests {
    use super::*;
    
    #[tokio::test]
    async fn test_optimized_processor_integration() {
        let config = OptimizationConfig::high_performance();
        let processor = OptimizedStreamProcessor::new(config);
        
        // æ¨¡æ“¬ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
        let (stream_sender, stream_receiver) = create_test_stream();
        let (response_sender, mut response_receiver) = mpsc::channel(100);
        
        // æœ€é©åŒ–å‡¦ç†é–‹å§‹
        let processor_task = tokio::spawn(async move {
            processor.process_stream_optimized(stream_receiver, response_sender).await;
        });
        
        // ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿é€ä¿¡
        send_test_requests(stream_sender, 1000).await;
        
        // ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ¤œè¨¼
        let mut response_count = 0;
        while let Some(response) = response_receiver.recv().await {
            response_count += 1;
            assert!(response.is_ok());
        }
        
        assert_eq!(response_count, 1000);
        
        processor_task.await.unwrap();
    }
    
    #[tokio::test]
    async fn test_performance_targets() {
        let processor = OptimizedStreamProcessor::new(
            OptimizationConfig::high_performance()
        );
        
        // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        let result = run_performance_benchmark(&processor).await;
        
        // ç›®æ¨™å€¤æ¤œè¨¼
        assert!(result.throughput >= 2000.0);
        assert!(result.p95_latency <= Duration::from_millis(25));
        assert!(result.cache_hit_ratio >= 0.7);
        assert!(result.worker_utilization >= 0.8);
    }
}
```

## æˆåŠŸåŸºæº–

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™
- **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: 2000 req/s ä»¥ä¸Š
- **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼**: P95 25msä»¥ä¸‹ã€P99 50msä»¥ä¸‹  
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: ç¾çŠ¶ã‹ã‚‰30%å‰Šæ¸›
- **åŒæ™‚æ¥ç¶š**: 100æ¥ç¶šå®‰å®šå‡¦ç†

### çµ±åˆåŸºæº–
- æ—¢å­˜ãƒ†ã‚¹ãƒˆ100%ãƒ‘ã‚¹
- å¾Œæ–¹äº’æ›æ€§ç¶­æŒ
- ã‚¨ãƒ©ãƒ¼ç‡1%ä»¥ä¸‹
- ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ãƒ‡ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

æœ€é©åŒ–ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼çµ±åˆå®Œäº†å¾Œï¼š
1. æœ¬ç•ªç’°å¢ƒã§ã®æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ
2. ç¶™ç¶šçš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã®è¨­å®š
3. æœ€é©åŒ–åŠ¹æœã®é•·æœŸè©•ä¾¡é–‹å§‹

## é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- Task 3.7 Fix 07-Aã€œE (å…¨ã‚µãƒ–ã‚¿ã‚¹ã‚¯)
- æ—¢å­˜service.rsãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœ
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ä»•æ§˜æ›¸