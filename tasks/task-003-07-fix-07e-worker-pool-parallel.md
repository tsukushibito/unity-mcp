# Task 3.7 Fix 07-E: ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«

## æ¦‚è¦
é«˜æ€§èƒ½ãªä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¾ã™ã€‚åŠ¹ç‡çš„ãªã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã€ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°ã€ãƒãƒƒãƒå‡¦ç†æ©Ÿèƒ½ã‚’çµ„ã¿åˆã‚ã›ã¦ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®å¤§å¹…ãªå‘ä¸Šã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®çŸ­ç¸®ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

## å„ªå…ˆåº¦
**ğŸ”´ æœ€é«˜å„ªå…ˆåº¦** - ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã®ä¸­æ ¸ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

## å®Ÿè£…æ™‚é–“è¦‹ç©ã‚‚ã‚Š
**75åˆ†** - é›†ä¸­ä½œæ¥­æ™‚é–“

## ä¾å­˜é–¢ä¿‚
- Task 3.7 Fix 07-A (åŸºç›¤ã‚¤ãƒ³ãƒ•ãƒ©æ•´å‚™) å®Œäº†å¿…é ˆ
- Task 3.7 Fix 07-B (ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ) å®Œäº†æ¨å¥¨

## å—ã‘å…¥ã‚ŒåŸºæº–

### ä¸¦åˆ—å‡¦ç†è¦ä»¶
- [ ] CPU ã‚³ã‚¢æ•°ã«åŸºã¥ãå‹•çš„ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°è¨­å®š
- [ ] åŠ¹ç‡çš„ãªã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°æ©Ÿèƒ½
- [ ] ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°
- [ ] ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæœ€é©åŒ–

### è² è·åˆ¶å¾¡è¦ä»¶
- [ ] ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼æ¤œå‡ºã¨åˆ¶å¾¡
- [ ] å„ªå…ˆåº¦ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯å‡¦ç†
- [ ] å‹•çš„ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°èª¿æ•´æ©Ÿèƒ½
- [ ] ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡ã®ç›£è¦–ã¨åˆ¶é™

### å®‰å®šæ€§è¦ä»¶
- [ ] ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ™‚è‡ªå‹•å¾©æ—§
- [ ] ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
- [ ] ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡ºã¨å›é¿
- [ ] ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ä¿è­·

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶
- [ ] ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ 2000 req/s é”æˆ
- [ ] ã‚¿ã‚¹ã‚¯åˆ†æ•£é…å»¶ < 1ms
- [ ] ãƒ¯ãƒ¼ã‚«ãƒ¼åˆ©ç”¨ç‡ > 80%
- [ ] ãƒãƒƒãƒå‡¦ç†åŠ¹ç‡ > 90%

## æŠ€è¡“çš„è©³ç´°

### WorkerPool å®Ÿè£…

#### src/grpc/performance/worker_pool.rs
```rust
//! é«˜æ€§èƒ½ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«
//! 
//! Unity MCP Server ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã«ãŠã„ã¦ã€ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹
//! ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“çŸ­ç¸®ã‚’å®Ÿç¾ã™ã‚‹ã€‚

use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use std::collections::{HashMap, VecDeque};
use tokio::sync::{mpsc, oneshot, Semaphore};
use tokio::task::JoinHandle;
use tracing::{debug, info, warn, error, instrument};
use uuid::Uuid;
use crate::grpc::service::UnityMcpServiceImpl;
use crate::grpc::performance::monitor::StreamPerformanceMonitor;
use crate::grpc::performance::resource_pool::ResourcePool;
use crate::unity::{StreamRequest, StreamResponse};

/// é«˜æ€§èƒ½ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«ã‚·ã‚¹ãƒ†ãƒ 
pub struct WorkerPool {
    // ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†
    workers: Arc<Mutex<Vec<Worker>>>,
    
    // ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
    task_scheduler: Arc<TaskScheduler>,
    
    // ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼
    load_balancer: Arc<LoadBalancer>,
    
    // ãƒãƒƒãƒãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼
    batch_processor: Arc<BatchProcessor>,
    
    // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
    performance_monitor: Option<Arc<StreamPerformanceMonitor>>,
    
    // è¨­å®š
    config: WorkerPoolConfig,
    
    // åˆ¶å¾¡ç”¨
    shutdown_signal: Arc<tokio::sync::Notify>,
    is_shutting_down: Arc<std::sync::atomic::AtomicBool>,
}

/// ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«è¨­å®š
#[derive(Debug, Clone)]
pub struct WorkerPoolConfig {
    // ãƒ¯ãƒ¼ã‚«ãƒ¼è¨­å®š
    pub worker_count: usize,
    pub worker_queue_capacity: usize,
    pub worker_restart_policy: WorkerRestartPolicy,
    
    // ãƒãƒƒãƒå‡¦ç†è¨­å®š
    pub batch_size: usize,
    pub batch_timeout: Duration,
    pub max_batch_wait: Duration,
    pub enable_adaptive_batching: bool,
    
    // è² è·åˆ¶å¾¡è¨­å®š
    pub backpressure_threshold: f64,
    pub backpressure_window: Duration,
    pub enable_dynamic_scaling: bool,
    pub max_worker_count: usize,
    pub min_worker_count: usize,
    
    // ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°è¨­å®š
    pub scheduling_strategy: SchedulingStrategy,
    pub task_priority_levels: usize,
    pub enable_task_stealing: bool,
    
    // ç›£è¦–è¨­å®š
    pub health_check_interval: Duration,
    pub performance_reporting_interval: Duration,
}

/// ãƒ¯ãƒ¼ã‚«ãƒ¼å†èµ·å‹•ãƒãƒªã‚·ãƒ¼
#[derive(Debug, Clone, Copy)]
pub enum WorkerRestartPolicy {
    Never,
    OnCrash,
    Periodic { interval: Duration },
}

/// ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥
#[derive(Debug, Clone, Copy)]
pub enum SchedulingStrategy {
    RoundRobin,
    LeastLoaded,
    Random,
    HashBased,
    PriorityBased,
}

impl Default for WorkerPoolConfig {
    fn default() -> Self {
        let cpu_count = num_cpus::get().max(2);
        
        Self {
            worker_count: cpu_count,
            worker_queue_capacity: 1000,
            worker_restart_policy: WorkerRestartPolicy::OnCrash,
            batch_size: 10,
            batch_timeout: Duration::from_millis(10),
            max_batch_wait: Duration::from_millis(50),
            enable_adaptive_batching: true,
            backpressure_threshold: 0.8,
            backpressure_window: Duration::from_secs(1),
            enable_dynamic_scaling: true,
            max_worker_count: cpu_count * 2,
            min_worker_count: 2,
            scheduling_strategy: SchedulingStrategy::LeastLoaded,
            task_priority_levels: 3,
            enable_task_stealing: true,
            health_check_interval: Duration::from_secs(10),
            performance_reporting_interval: Duration::from_secs(5),
        }
    }
}

/// å€‹åˆ¥ãƒ¯ãƒ¼ã‚«ãƒ¼å®Ÿè£…
pub struct Worker {
    // è­˜åˆ¥å­
    pub id: usize,
    pub uuid: Uuid,
    
    // ã‚¿ã‚¹ã‚¯å‡¦ç†
    task_receiver: mpsc::Receiver<ProcessingTask>,
    
    // ã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    service: Arc<UnityMcpServiceImpl>,
    
    // çµ±è¨ˆ
    stats: Arc<Mutex<WorkerStatistics>>,
    
    // åˆ¶å¾¡
    handle: Option<JoinHandle<()>>,
    health_status: Arc<Mutex<WorkerHealthStatus>>,
    
    // ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«
    resource_pool: Option<Arc<ResourcePool>>,
}

/// å‡¦ç†ã‚¿ã‚¹ã‚¯
#[derive(Debug)]
pub struct ProcessingTask {
    pub task_id: Uuid,
    pub request: StreamRequest,
    pub response_sender: oneshot::Sender<Result<StreamResponse, ProcessingError>>,
    pub priority: TaskPriority,
    pub submitted_at: Instant,
    pub context: TaskContext,
}

/// ã‚¿ã‚¹ã‚¯å„ªå…ˆåº¦
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
pub enum TaskPriority {
    Low = 0,
    Normal = 1,  
    High = 2,
    Critical = 3,
}

/// ã‚¿ã‚¹ã‚¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
#[derive(Debug, Clone)]
pub struct TaskContext {
    pub connection_id: String,
    pub message_id: u64,
    pub batch_id: Option<Uuid>,
    pub deadline: Option<Instant>,
}

/// ãƒ¯ãƒ¼ã‚«ãƒ¼çµ±è¨ˆ
#[derive(Debug, Default, Clone)]
pub struct WorkerStatistics {
    pub tasks_processed: u64,
    pub tasks_failed: u64,
    pub total_processing_time: Duration,
    pub avg_processing_time: Duration,
    pub current_queue_size: usize,
    pub last_activity: Option<Instant>,
}

/// ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ˜ãƒ«ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
#[derive(Debug, Clone)]
pub struct WorkerHealthStatus {
    pub is_healthy: bool,
    pub last_heartbeat: Instant,
    pub error_count: u32,
    pub last_error: Option<String>,
}

/// ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
pub struct TaskScheduler {
    // ãƒ¯ãƒ¼ã‚«ãƒ¼é€šä¿¡ãƒãƒ£ãƒãƒ«
    worker_senders: Arc<Mutex<HashMap<usize, mpsc::Sender<ProcessingTask>>>>,
    
    // è² è·è¿½è·¡
    worker_loads: Arc<Mutex<HashMap<usize, f64>>>,
    
    // å„ªå…ˆåº¦ã‚­ãƒ¥ãƒ¼
    priority_queues: Arc<Mutex<HashMap<TaskPriority, VecDeque<ProcessingTask>>>>,
    
    // ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°çµ±è¨ˆ
    scheduling_stats: Arc<Mutex<SchedulingStatistics>>,
    
    // è¨­å®š
    strategy: SchedulingStrategy,
}

/// ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°çµ±è¨ˆ
#[derive(Debug, Default, Clone)]
pub struct SchedulingStatistics {
    pub total_scheduled: u64,
    pub avg_scheduling_time: Duration,
    pub load_balance_efficiency: f64,
    pub priority_distribution: HashMap<TaskPriority, u64>,
}

/// ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼
pub struct LoadBalancer {
    // ãƒ¯ãƒ¼ã‚«ãƒ¼è² è·ç›£è¦–
    worker_metrics: Arc<Mutex<HashMap<usize, WorkerMetrics>>>,
    
    // è² è·åˆ†æ•£æˆ¦ç•¥
    strategy: LoadBalancingStrategy,
    
    // é©å¿œçš„èª¿æ•´
    adaptive_controller: Option<AdaptiveLoadController>,
}

/// ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹
#[derive(Debug, Clone)]
pub struct WorkerMetrics {
    pub current_load: f64,          // 0.0-1.0
    pub queue_depth: usize,
    pub avg_response_time: Duration,
    pub throughput: f64,            // req/s
    pub error_rate: f64,            // 0.0-1.0
    pub last_update: Instant,
}

/// è² è·åˆ†æ•£æˆ¦ç•¥
#[derive(Debug, Clone, Copy)]
pub enum LoadBalancingStrategy {
    RoundRobin,
    LeastConnections,
    WeightedRoundRobin,
    ResponseTimeWeighted,
    AdaptiveHybrid,
}

/// ãƒãƒƒãƒãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼
pub struct BatchProcessor {
    // ãƒãƒƒãƒãƒ³ã‚°ã‚­ãƒ¥ãƒ¼
    batching_queues: Arc<Mutex<HashMap<String, BatchingQueue>>>,
    
    // ãƒãƒƒãƒçµ±è¨ˆ
    batch_stats: Arc<Mutex<BatchStatistics>>,
    
    // è¨­å®š
    config: BatchProcessorConfig,
}

/// ãƒãƒƒãƒãƒ³ã‚°ã‚­ãƒ¥ãƒ¼
#[derive(Debug)]
pub struct BatchingQueue {
    pub tasks: VecDeque<ProcessingTask>,
    pub created_at: Instant,
    pub last_flush: Instant,
    pub target_size: usize,
}

/// ãƒãƒƒãƒçµ±è¨ˆ
#[derive(Debug, Default, Clone)]
pub struct BatchStatistics {
    pub total_batches: u64,
    pub avg_batch_size: f64,
    pub batch_efficiency: f64,
    pub total_batch_processing_time: Duration,
}

/// ãƒãƒƒãƒãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼è¨­å®š
#[derive(Debug, Clone)]
pub struct BatchProcessorConfig {
    pub default_batch_size: usize,
    pub batch_timeout: Duration,
    pub max_batch_wait: Duration,
    pub enable_adaptive_sizing: bool,
}

impl WorkerPool {
    /// æ–°ã—ã„ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆ
    pub fn new() -> Self {
        Self::with_config(WorkerPoolConfig::default())
    }

    /// è¨­å®šä»˜ãã§ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆ
    pub fn with_config(config: WorkerPoolConfig) -> Self {
        info!("Initializing worker pool with {} workers", config.worker_count);

        let task_scheduler = Arc::new(TaskScheduler::new(config.scheduling_strategy));
        let load_balancer = Arc::new(LoadBalancer::new(LoadBalancingStrategy::AdaptiveHybrid));
        let batch_processor = Arc::new(BatchProcessor::new(BatchProcessorConfig {
            default_batch_size: config.batch_size,
            batch_timeout: config.batch_timeout,
            max_batch_wait: config.max_batch_wait,
            enable_adaptive_sizing: config.enable_adaptive_batching,
        }));

        let shutdown_signal = Arc::new(tokio::sync::Notify::new());
        let is_shutting_down = Arc::new(std::sync::atomic::AtomicBool::new(false));

        let pool = Self {
            workers: Arc::new(Mutex::new(Vec::new())),
            task_scheduler,
            load_balancer,
            batch_processor,
            performance_monitor: None,
            config: config.clone(),
            shutdown_signal,
            is_shutting_down,
        };

        // ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’åˆæœŸåŒ–
        pool.initialize_workers();

        // ç›£è¦–ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
        pool.start_monitoring_tasks();

        info!("Worker pool initialized successfully");
        pool
    }

    /// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’è¨­å®š
    pub fn with_performance_monitor(mut self, monitor: Arc<StreamPerformanceMonitor>) -> Self {
        self.performance_monitor = Some(monitor);
        self
    }

    /// ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œï¼ˆéåŒæœŸï¼‰
    #[instrument(skip(self, request))]
    pub async fn execute(
        &self,
        request: StreamRequest,
        priority: TaskPriority,
        context: TaskContext,
    ) -> Result<StreamResponse, ProcessingError> {
        let start_time = Instant::now();
        let task_id = Uuid::new_v4();

        // ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ä¸­ãƒã‚§ãƒƒã‚¯
        if self.is_shutting_down.load(std::sync::atomic::Ordering::Relaxed) {
            return Err(ProcessingError::PoolShuttingDown);
        }

        debug!(
            task_id = %task_id,
            priority = ?priority,
            connection_id = %context.connection_id,
            "Submitting task for execution"
        );

        // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”¨ãƒãƒ£ãƒãƒ«ä½œæˆ
        let (response_sender, response_receiver) = oneshot::channel();

        let task = ProcessingTask {
            task_id,
            request,
            response_sender,
            priority,
            submitted_at: start_time,
            context,
        };

        // ãƒãƒƒãƒå‡¦ç†ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯
        if self.should_batch_task(&task) {
            self.batch_processor.submit_task(task).await?;
        } else {
            // ç›´æ¥ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
            self.schedule_task(task).await?;
        }

        // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å¾…æ©Ÿ
        match response_receiver.await {
            Ok(result) => {
                let processing_time = start_time.elapsed();
                self.record_task_completion(task_id, &result, processing_time);
                result
            }
            Err(_) => {
                warn!(task_id = %task_id, "Task response channel closed");
                Err(ProcessingError::TaskCancelled)
            }
        }
    }

    /// ãƒãƒƒãƒã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
    #[instrument(skip(self, tasks))]
    pub async fn execute_batch(
        &self,
        tasks: Vec<ProcessingTask>,
    ) -> Vec<Result<StreamResponse, ProcessingError>> {
        let batch_id = Uuid::new_v4();
        let batch_size = tasks.len();
        let start_time = Instant::now();

        debug!(
            batch_id = %batch_id,
            batch_size = batch_size,
            "Processing batch of tasks"
        );

        // æœ€é©ãªãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’é¸æŠ
        let worker_id = self.load_balancer.select_worker_for_batch(&tasks).await;

        // ãƒãƒƒãƒã‚’ä¸¦åˆ—å‡¦ç†
        let results = self.process_batch_parallel(worker_id, tasks).await;

        let batch_time = start_time.elapsed();
        self.record_batch_completion(batch_id, batch_size, batch_time);

        results
    }

    /// ç¾åœ¨ã®ãƒ—ãƒ¼ãƒ«ä½¿ç”¨ç‡ã‚’å–å¾—
    pub async fn usage_ratio(&self) -> f64 {
        let workers = match self.workers.lock() {
            Ok(workers) => workers,
            Err(_) => return 0.0,
        };

        let total_capacity = workers.len() as f64;
        if total_capacity == 0.0 {
            return 0.0;
        }

        let busy_count = workers
            .iter()
            .filter(|worker| self.is_worker_busy(worker))
            .count() as f64;

        busy_count / total_capacity
    }

    /// ãƒ—ãƒ¼ãƒ«çµ±è¨ˆã‚’å–å¾—
    pub fn get_pool_statistics(&self) -> WorkerPoolStatistics {
        let workers = self.workers.lock().unwrap();
        let total_workers = workers.len();
        
        let mut total_processed = 0u64;
        let mut total_failed = 0u64;
        let mut total_processing_time = Duration::default();
        let mut active_workers = 0usize;

        for worker in workers.iter() {
            if let Ok(stats) = worker.stats.lock() {
                total_processed += stats.tasks_processed;
                total_failed += stats.tasks_failed;
                total_processing_time += stats.total_processing_time;
                
                if stats.current_queue_size > 0 {
                    active_workers += 1;
                }
            }
        }

        let avg_processing_time = if total_processed > 0 {
            total_processing_time / total_processed as u32
        } else {
            Duration::default()
        };

        WorkerPoolStatistics {
            total_workers,
            active_workers,
            total_tasks_processed: total_processed,
            total_tasks_failed: total_failed,
            success_rate: if total_processed > 0 {
                (total_processed - total_failed) as f64 / total_processed as f64
            } else {
                1.0
            },
            avg_processing_time,
            current_usage_ratio: active_workers as f64 / total_workers as f64,
            throughput: self.calculate_current_throughput(),
            batch_statistics: self.batch_processor.get_statistics(),
        }
    }

    /// ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
    pub async fn shutdown(&self) -> Result<(), ProcessingError> {
        info!("Initiating worker pool shutdown");
        
        self.is_shutting_down
            .store(true, std::sync::atomic::Ordering::Relaxed);

        // æ–°è¦ã‚¿ã‚¹ã‚¯ã®å—ä»˜åœæ­¢
        self.shutdown_signal.notify_waiters();

        // é€²è¡Œä¸­ã‚¿ã‚¹ã‚¯ã®å®Œäº†ã‚’å¾…æ©Ÿ
        let timeout = Duration::from_secs(30);
        self.wait_for_completion(timeout).await?;

        // ãƒ¯ãƒ¼ã‚«ãƒ¼ã®åœæ­¢
        self.stop_all_workers().await;

        info!("Worker pool shutdown completed");
        Ok(())
    }

    // å†…éƒ¨å®Ÿè£…ãƒ¡ã‚½ãƒƒãƒ‰

    fn initialize_workers(&self) {
        let mut workers = self.workers.lock().unwrap();
        
        for i in 0..self.config.worker_count {
            let worker = self.create_worker(i);
            workers.push(worker);
        }
        
        info!("Initialized {} workers", self.config.worker_count);
    }

    fn create_worker(&self, id: usize) -> Worker {
        let (task_sender, task_receiver) = mpsc::channel(self.config.worker_queue_capacity);
        let service = Arc::new(UnityMcpServiceImpl::new());
        let stats = Arc::new(Mutex::new(WorkerStatistics::default()));
        let health_status = Arc::new(Mutex::new(WorkerHealthStatus {
            is_healthy: true,
            last_heartbeat: Instant::now(),
            error_count: 0,
            last_error: None,
        }));

        // ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã«ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ç™»éŒ²
        self.task_scheduler.register_worker(id, task_sender);

        let worker = Worker {
            id,
            uuid: Uuid::new_v4(),
            task_receiver,
            service,
            stats: Arc::clone(&stats),
            handle: None,
            health_status,
            resource_pool: None,
        };

        // ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹
        self.start_worker_loop(&worker);

        worker
    }

    fn start_worker_loop(&self, worker: &Worker) {
        let worker_id = worker.id;
        let mut task_receiver = std::mem::take(&mut worker.task_receiver);
        let service = Arc::clone(&worker.service);
        let stats = Arc::clone(&worker.stats);
        let shutdown_signal = Arc::clone(&self.shutdown_signal);

        let handle = tokio::spawn(async move {
            info!(worker_id = worker_id, "Worker loop started");

            loop {
                tokio::select! {
                    task = task_receiver.recv() => {
                        match task {
                            Some(processing_task) => {
                                Self::process_single_task(
                                    &service,
                                    processing_task,
                                    Arc::clone(&stats),
                                ).await;
                            }
                            None => {
                                warn!(worker_id = worker_id, "Task channel closed");
                                break;
                            }
                        }
                    }
                    _ = shutdown_signal.notified() => {
                        info!(worker_id = worker_id, "Worker received shutdown signal");
                        break;
                    }
                }
            }

            info!(worker_id = worker_id, "Worker loop terminated");
        });

        // ãƒãƒ³ãƒ‰ãƒ«ã‚’ä¿å­˜ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ãªæ–¹æ³•ã§ä¿å­˜ï¼‰
        debug!(worker_id = worker_id, "Worker loop handle created");
    }

    async fn process_single_task(
        service: &Arc<UnityMcpServiceImpl>,
        task: ProcessingTask,
        stats: Arc<Mutex<WorkerStatistics>>,
    ) {
        let start_time = Instant::now();
        let task_id = task.task_id;

        debug!(
            task_id = %task_id,
            worker_service = "processing",
            "Processing task"
        );

        // å®Ÿéš›ã®ã‚¿ã‚¹ã‚¯å‡¦ç†ï¼ˆè©³ç´°å®Ÿè£…ã¯çœç•¥ï¼‰
        let result = Self::execute_stream_request(service, &task.request).await;

        let processing_time = start_time.elapsed();

        // çµæœã‚’é€ä¿¡
        let send_result = task.response_sender.send(result.clone());
        if send_result.is_err() {
            warn!(task_id = %task_id, "Failed to send task result - receiver dropped");
        }

        // çµ±è¨ˆæ›´æ–°
        if let Ok(mut worker_stats) = stats.lock() {
            worker_stats.tasks_processed += 1;
            if result.is_err() {
                worker_stats.tasks_failed += 1;
            }
            worker_stats.total_processing_time += processing_time;
            worker_stats.avg_processing_time = 
                worker_stats.total_processing_time / worker_stats.tasks_processed as u32;
            worker_stats.last_activity = Some(Instant::now());
        }

        debug!(
            task_id = %task_id,
            processing_time = ?processing_time,
            success = result.is_ok(),
            "Task processing completed"
        );
    }

    async fn execute_stream_request(
        _service: &Arc<UnityMcpServiceImpl>,
        _request: &StreamRequest,
    ) -> Result<StreamResponse, ProcessingError> {
        // å®Ÿéš›ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†
        // è©³ç´°å®Ÿè£…ã¯çœç•¥
        Err(ProcessingError::NotImplemented)
    }

    async fn schedule_task(&self, task: ProcessingTask) -> Result<(), ProcessingError> {
        self.task_scheduler.schedule(task).await
    }

    fn should_batch_task(&self, _task: &ProcessingTask) -> bool {
        // ãƒãƒƒãƒå‡¦ç†åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
        false // ç°¡ç•¥åŒ–
    }

    async fn process_batch_parallel(
        &self,
        _worker_id: usize,
        _tasks: Vec<ProcessingTask>,
    ) -> Vec<Result<StreamResponse, ProcessingError>> {
        // ãƒãƒƒãƒä¸¦åˆ—å‡¦ç†ã®å®Ÿè£…
        Vec::new() // ç°¡ç•¥åŒ–
    }

    fn is_worker_busy(&self, _worker: &Worker) -> bool {
        // ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ãƒ“ã‚¸ãƒ¼çŠ¶æ…‹åˆ¤å®š
        false // ç°¡ç•¥åŒ–
    }

    fn calculate_current_throughput(&self) -> f64 {
        // ç¾åœ¨ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
        0.0 // ç°¡ç•¥åŒ–
    }

    fn record_task_completion(
        &self,
        _task_id: Uuid,
        _result: &Result<StreamResponse, ProcessingError>,
        _processing_time: Duration,
    ) {
        // ã‚¿ã‚¹ã‚¯å®Œäº†è¨˜éŒ²
    }

    fn record_batch_completion(&self, _batch_id: Uuid, _batch_size: usize, _batch_time: Duration) {
        // ãƒãƒƒãƒå®Œäº†è¨˜éŒ²
    }

    fn start_monitoring_tasks(&self) {
        // ç›£è¦–ã‚¿ã‚¹ã‚¯é–‹å§‹
    }

    async fn wait_for_completion(&self, _timeout: Duration) -> Result<(), ProcessingError> {
        // å®Œäº†å¾…æ©Ÿ
        Ok(())
    }

    async fn stop_all_workers(&self) {
        // å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼åœæ­¢
    }
}

/// ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«çµ±è¨ˆ
#[derive(Debug, Clone)]
pub struct WorkerPoolStatistics {
    pub total_workers: usize,
    pub active_workers: usize,
    pub total_tasks_processed: u64,
    pub total_tasks_failed: u64,
    pub success_rate: f64,
    pub avg_processing_time: Duration,
    pub current_usage_ratio: f64,
    pub throughput: f64,
    pub batch_statistics: BatchStatistics,
}

/// å‡¦ç†ã‚¨ãƒ©ãƒ¼
#[derive(Debug, thiserror::Error)]
pub enum ProcessingError {
    #[error("Task processing failed: {0}")]
    TaskFailed(String),
    
    #[error("Worker pool is shutting down")]
    PoolShuttingDown,
    
    #[error("Task was cancelled")]
    TaskCancelled,
    
    #[error("No available workers")]
    NoAvailableWorkers,
    
    #[error("Task timeout")]
    TaskTimeout,
    
    #[error("Not implemented")]
    NotImplemented,
}

// çœç•¥ã•ã‚ŒãŸå®Ÿè£…ã‚¯ãƒ©ã‚¹ã®ã‚¹ã‚¿ãƒ–...

impl TaskScheduler {
    fn new(_strategy: SchedulingStrategy) -> Self {
        Self {
            worker_senders: Arc::new(Mutex::new(HashMap::new())),
            worker_loads: Arc::new(Mutex::new(HashMap::new())),
            priority_queues: Arc::new(Mutex::new(HashMap::new())),
            scheduling_stats: Arc::new(Mutex::new(SchedulingStatistics::default())),
            strategy: _strategy,
        }
    }

    fn register_worker(&self, _id: usize, _sender: mpsc::Sender<ProcessingTask>) {
        // ãƒ¯ãƒ¼ã‚«ãƒ¼ç™»éŒ²å‡¦ç†
    }

    async fn schedule(&self, _task: ProcessingTask) -> Result<(), ProcessingError> {
        // ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°å‡¦ç†
        Ok(())
    }
}

impl LoadBalancer {
    fn new(_strategy: LoadBalancingStrategy) -> Self {
        Self {
            worker_metrics: Arc::new(Mutex::new(HashMap::new())),
            strategy: _strategy,
            adaptive_controller: None,
        }
    }

    async fn select_worker_for_batch(&self, _tasks: &[ProcessingTask]) -> usize {
        // ãƒãƒƒãƒç”¨ãƒ¯ãƒ¼ã‚«ãƒ¼é¸æŠ
        0
    }
}

impl BatchProcessor {
    fn new(_config: BatchProcessorConfig) -> Self {
        Self {
            batching_queues: Arc::new(Mutex::new(HashMap::new())),
            batch_stats: Arc::new(Mutex::new(BatchStatistics::default())),
            config: _config,
        }
    }

    async fn submit_task(&self, _task: ProcessingTask) -> Result<(), ProcessingError> {
        // ãƒãƒƒãƒã‚¿ã‚¹ã‚¯é€ä¿¡
        Ok(())
    }

    fn get_statistics(&self) -> BatchStatistics {
        self.batch_stats.lock()
            .map(|stats| stats.clone())
            .unwrap_or_default()
    }
}

// çœç•¥ã•ã‚ŒãŸæ§‹é€ ä½“...
pub struct AdaptiveLoadController;
```

## å®Ÿè£…è¨ˆç”»

### Step 1: åŸºæœ¬ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ« (30åˆ†)
1. WorkerPool åŸºæœ¬æ§‹é€ 
2. Worker å€‹åˆ¥å®Ÿè£…
3. åŸºæœ¬çš„ãªã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ©Ÿèƒ½

### Step 2: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ  (25åˆ†)
1. TaskScheduler å®Ÿè£…
2. ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°æ©Ÿèƒ½
3. å„ªå…ˆåº¦ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯å‡¦ç†

### Step 3: ãƒãƒƒãƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ  (20åˆ†)
1. BatchProcessor å®Ÿè£…
2. é©å¿œçš„ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´
3. ãƒãƒƒãƒåŠ¹ç‡æœ€é©åŒ–

## ãƒ†ã‚¹ãƒˆè¦ä»¶

### ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«å‹•ä½œãƒ†ã‚¹ãƒˆ
```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_worker_pool_basic_execution() {
        let pool = WorkerPool::new();
        
        let request = create_test_stream_request();
        let context = TaskContext {
            connection_id: "test-conn".to_string(),
            message_id: 1,
            batch_id: None,
            deadline: None,
        };

        let result = pool.execute(
            request, 
            TaskPriority::Normal, 
            context
        ).await;

        // çµæœæ¤œè¨¼ï¼ˆå®Ÿè£…å®Œäº†å¾Œï¼‰
        // assert!(result.is_ok());
    }

    #[tokio::test]
    async fn test_batch_processing() {
        let pool = WorkerPool::new();
        
        let tasks: Vec<ProcessingTask> = (0..10)
            .map(|i| create_test_task(i))
            .collect();

        let results = pool.execute_batch(tasks).await;
        assert_eq!(results.len(), 10);
    }

    #[tokio::test]
    async fn test_worker_pool_statistics() {
        let pool = WorkerPool::new();
        let stats = pool.get_pool_statistics();
        
        assert!(stats.total_workers > 0);
        assert_eq!(stats.total_tasks_processed, 0);
    }
}
```

## æˆåŠŸåŸºæº–

### æ©Ÿèƒ½åŸºæº–
- ä¸¦åˆ—ã‚¿ã‚¹ã‚¯å‡¦ç†ãŒæ­£å¸¸å‹•ä½œ
- ãƒãƒƒãƒå‡¦ç†åŠ¹ç‡ > 90%
- ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½
- ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–
- ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ > 2000 req/s
- ãƒ¯ãƒ¼ã‚«ãƒ¼åˆ©ç”¨ç‡ > 80%
- ã‚¿ã‚¹ã‚¯åˆ†æ•£é…å»¶ < 1ms
- ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡æ©Ÿèƒ½

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«å®Œäº†å¾Œï¼š
1. Task 3.7 Fix 07-F: æœ€é©åŒ–ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼çµ±åˆå®Ÿè£…
2. å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆãƒ†ã‚¹ãƒˆ
3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ

## é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- Task 3.7 Fix 07-A (åŸºç›¤ã‚¤ãƒ³ãƒ•ãƒ©æ•´å‚™)
- Task 3.7 Fix 07-B (ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ )
- Rust ä¸¦è¡Œãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹