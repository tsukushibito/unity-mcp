# Task 3.7 Fix 07-E: ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«ï¼ˆRayonæ´»ç”¨ç‰ˆï¼‰

## æ¦‚è¦
Rayonã‚¯ãƒ¬ãƒ¼ãƒˆã‚’æ´»ç”¨ã—ãŸé«˜æ€§èƒ½ä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¾ã™ã€‚Rayonã®å®Ÿè¨¼æ¸ˆã¿work-stealingã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨CPUæœ€é©åŒ–ã•ã‚ŒãŸã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚·ãƒ³ãƒ—ãƒ«ã‹ã¤åŠ¹ç‡çš„ãªä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“çŸ­ç¸®ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

## å„ªå…ˆåº¦
**ğŸ”´ æœ€é«˜å„ªå…ˆåº¦** - ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã®ä¸­æ ¸ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

## å®Ÿè£…æ™‚é–“è¦‹ç©ã‚‚ã‚Š
**30åˆ†** - é›†ä¸­ä½œæ¥­æ™‚é–“ï¼ˆRayonæ´»ç”¨ã«ã‚ˆã‚Šå¤§å¹…çŸ­ç¸®ï¼‰

## ä¾å­˜é–¢ä¿‚
- Task 3.7 Fix 07-A (åŸºç›¤ã‚¤ãƒ³ãƒ•ãƒ©æ•´å‚™) å®Œäº†å¿…é ˆ
- Task 3.7 Fix 07-B (ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ) å®Œäº†æ¨å¥¨

## å—ã‘å…¥ã‚ŒåŸºæº–

### ä¸¦åˆ—å‡¦ç†è¦ä»¶
- [ ] Rayonã«ã‚ˆã‚‹è‡ªå‹•CPUæœ€é©åŒ–
- [ ] Work-stealingã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹åŠ¹ç‡çš„è² è·åˆ†æ•£
- [ ] Parallel iteratorã«ã‚ˆã‚‹ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–
- [ ] ã‚«ã‚¹ã‚¿ãƒ ThreadPoolBuilderè¨­å®š

### è² è·åˆ¶å¾¡è¦ä»¶
- [ ] ã‚»ãƒãƒ•ã‚©ã«ã‚ˆã‚‹ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡
- [ ] éåŒæœŸã‚¿ã‚¹ã‚¯ã¨ã®çµ±åˆï¼ˆtokio-rayonï¼‰
- [ ] ã‚·ãƒ³ãƒ—ãƒ«ãªå„ªå…ˆåº¦åˆ¶å¾¡
- [ ] çµ±è¨ˆåé›†ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–çµ±åˆ

### å®‰å®šæ€§è¦ä»¶
- [ ] Rayonã®å†…è”µãƒ‘ãƒ‹ãƒƒã‚¯å¾©æ—§æ©Ÿèƒ½æ´»ç”¨
- [ ] ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
- [ ] Rustã®å‹å®‰å…¨æ€§ã«ã‚ˆã‚‹ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯å›é¿
- [ ] è»½é‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶
- [ ] ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ 2000 req/s é”æˆ
- [ ] ã‚¿ã‚¹ã‚¯åˆ†æ•£é…å»¶ < 1ms
- [ ] ãƒ¯ãƒ¼ã‚«ãƒ¼åˆ©ç”¨ç‡ > 80%
- [ ] ãƒãƒƒãƒå‡¦ç†åŠ¹ç‡ > 90%

## æŠ€è¡“çš„è©³ç´°

### ParallelProcessor å®Ÿè£…ï¼ˆRayonæ´»ç”¨ï¼‰

#### server/Cargo.toml ã¸ã®ä¾å­˜è¿½åŠ 
```toml
[dependencies]
rayon = "1.8"
tokio-rayon = "2.1"  # tokioã¨ã®çµ±åˆç”¨
```

#### src/grpc/performance/parallel_processor.rs
```rust
//! Rayonæ´»ç”¨ä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
//! 
//! Unity MCP Server ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã«ãŠã„ã¦ã€Rayonã®å®Ÿè¨¼æ¸ˆã¿
//! work-stealingã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹é«˜åŠ¹ç‡ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿç¾ã™ã‚‹ã€‚

use std::sync::{Arc, Mutex, atomic::{AtomicU64, Ordering}};
use std::time::{Duration, Instant};
use rayon::prelude::*;
use rayon::{ThreadPool, ThreadPoolBuilder};
use tokio::sync::Semaphore;
use tracing::{debug, info, warn, error, instrument};
use uuid::Uuid;
use crate::grpc::service::UnityMcpServiceImpl;
use crate::grpc::performance::monitor::StreamPerformanceMonitor;
use crate::unity::{StreamRequest, StreamResponse};

/// Rayonä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
pub struct ParallelProcessor {
    // Rayonã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«
    thread_pool: ThreadPool,
    
    // éåŒæœŸåˆ¶å¾¡
    semaphore: Arc<Semaphore>,
    
    // çµ±è¨ˆæƒ…å ±
    stats: Arc<ProcessingStatistics>,
    
    // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
    performance_monitor: Option<Arc<StreamPerformanceMonitor>>,
    
    // è¨­å®š
    config: ParallelConfig,
}

/// ä¸¦åˆ—å‡¦ç†è¨­å®š
#[derive(Debug, Clone)]
pub struct ParallelConfig {
    // Rayonã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«è¨­å®š
    pub thread_count: Option<usize>,  // Noneã®å ´åˆã¯CPUã‚³ã‚¢æ•°è‡ªå‹•è¨­å®š
    pub thread_name_prefix: String,
    
    // ãƒãƒƒãƒå‡¦ç†è¨­å®š
    pub batch_size: usize,
    pub max_concurrent_batches: usize,
    
    // ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡
    pub max_pending_tasks: usize,
    
    // ç›£è¦–è¨­å®š
    pub enable_statistics: bool,
}

impl Default for ParallelConfig {
    fn default() -> Self {
        Self {
            thread_count: None,  // RayonãŒè‡ªå‹•è¨­å®š
            thread_name_prefix: "unity-mcp-worker".to_string(),
            batch_size: 10,
            max_concurrent_batches: 100,
            max_pending_tasks: 1000,
            enable_statistics: true,
        }
    }
}

/// å‡¦ç†çµ±è¨ˆ
#[derive(Debug, Default)]
pub struct ProcessingStatistics {
    pub total_processed: AtomicU64,
    pub total_failed: AtomicU64,
    pub total_processing_time: Mutex<Duration>,
}

impl ParallelProcessor {
    /// æ–°ã—ã„ä¸¦åˆ—ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’ä½œæˆ
    pub fn new() -> anyhow::Result<Self> {
        Self::with_config(ParallelConfig::default())
    }

    /// è¨­å®šä»˜ãã§ä¸¦åˆ—ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’ä½œæˆ
    pub fn with_config(config: ParallelConfig) -> anyhow::Result<Self> {
        let mut builder = ThreadPoolBuilder::new()
            .thread_name(|i| format!("{}-{}", config.thread_name_prefix, i));
        
        if let Some(count) = config.thread_count {
            builder = builder.num_threads(count);
        }
        
        let thread_pool = builder.build()?;
        let semaphore = Arc::new(Semaphore::new(config.max_pending_tasks));
        let stats = Arc::new(ProcessingStatistics::default());
        
        info!(
            "Parallel processor initialized with {} threads", 
            thread_pool.current_num_threads()
        );
        
        Ok(Self {
            thread_pool,
            semaphore,
            stats,
            performance_monitor: None,
            config,
        })
    }

    /// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’è¨­å®š
    pub fn with_performance_monitor(mut self, monitor: Arc<StreamPerformanceMonitor>) -> Self {
        self.performance_monitor = Some(monitor);
        self
    }

    /// ä¸¦åˆ—ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œ
    #[instrument(skip(self, requests))]
    pub async fn execute_parallel_batch(
        &self,
        requests: Vec<StreamRequest>,
    ) -> Vec<Result<StreamResponse, ProcessingError>> {
        let batch_size = requests.len();
        let start_time = Instant::now();
        
        debug!("Processing batch of {} requests in parallel", batch_size);
        
        // ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡
        let _permit = self.semaphore.acquire_many(batch_size as u32).await
            .map_err(|_| ProcessingError::BackpressureExceeded)?;
        
        // Rayonã§ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
        let results = self.thread_pool.install(|| {
            requests.par_iter()
                .map(|request| self.process_single_request(request))
                .collect::<Vec<_>>()
        });
        
        let processing_time = start_time.elapsed();
        self.record_batch_completion(batch_size, processing_time);
        
        Ok(results)
    }

    /// ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãƒãƒƒãƒå‡¦ç†
    pub async fn execute_chunked_parallel(
        &self,
        requests: Vec<StreamRequest>,
    ) -> Vec<Result<StreamResponse, ProcessingError>> {
        let chunk_size = self.config.batch_size;
        let start_time = Instant::now();
        
        let results: Vec<_> = requests
            .par_chunks(chunk_size)
            .flat_map(|chunk| {
                chunk.par_iter()
                    .map(|request| self.process_single_request(request))
            })
            .collect();
        
        let processing_time = start_time.elapsed();
        self.record_batch_completion(requests.len(), processing_time);
        
        results
    }

    /// å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ï¼ˆå†…éƒ¨å®Ÿè£…ï¼‰
    fn process_single_request(&self, request: &StreamRequest) -> Result<StreamResponse, ProcessingError> {
        let start_time = Instant::now();
        
        // å®Ÿéš›ã®å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå®Ÿè£…æ™‚ã«è©³ç´°åŒ–ï¼‰
        let result = self.execute_request_sync(request);
        
        // çµ±è¨ˆæ›´æ–°
        if self.config.enable_statistics {
            self.update_statistics(&result, start_time.elapsed());
        }
        
        result
    }

    /// åŒæœŸãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
    fn execute_request_sync(&self, _request: &StreamRequest) -> Result<StreamResponse, ProcessingError> {
        // TODO: å®Ÿéš›ã®ã‚µãƒ¼ãƒ“ã‚¹å‘¼ã³å‡ºã—å®Ÿè£…
        Err(ProcessingError::NotImplemented)
    }

    /// çµ±è¨ˆæ›´æ–°
    fn update_statistics(&self, result: &Result<StreamResponse, ProcessingError>, duration: Duration) {
        self.stats.total_processed.fetch_add(1, Ordering::Relaxed);
        if result.is_err() {
            self.stats.total_failed.fetch_add(1, Ordering::Relaxed);
        }
        
        if let Ok(mut total_time) = self.stats.total_processing_time.lock() {
            *total_time += duration;
        }
    }

    /// ãƒãƒƒãƒå®Œäº†è¨˜éŒ²
    fn record_batch_completion(&self, batch_size: usize, duration: Duration) {
        debug!(
            "Batch completed: {} requests in {:?}",
            batch_size, duration
        );
        
        if let Some(monitor) = &self.performance_monitor {
            // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã«è¨˜éŒ²
        }
    }
}

/// å‡¦ç†ã‚¨ãƒ©ãƒ¼
#[derive(Debug, thiserror::Error)]
pub enum ProcessingError {
    #[error("Task processing failed: {0}")]
    TaskFailed(String),
    
    #[error("Backpressure limit exceeded")]
    BackpressureExceeded,
    
    #[error("Thread pool error: {0}")]
    ThreadPoolError(String),
    
    #[error("Not implemented")]
    NotImplemented,
}
```

## å®Ÿè£…è¨ˆç”»ï¼ˆRayonæ´»ç”¨ç‰ˆï¼‰

### Step 1: Rayonçµ±åˆã¨Cargoè¨­å®š (10åˆ†)
1. `Cargo.toml` ã«rayonä¾å­˜é–¢ä¿‚è¿½åŠ 
2. `ParallelProcessor` åŸºæœ¬æ§‹é€ å®Ÿè£…
3. ThreadPoolBuilderè¨­å®š

### Step 2: ä¸¦åˆ—å‡¦ç†æ©Ÿèƒ½å®Ÿè£… (15åˆ†)
1. `execute_parallel_batch()` ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè£…
2. `execute_chunked_parallel()` ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè£…
3. ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡ï¼ˆSemaphoreï¼‰

### Step 3: çµ±è¨ˆã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°çµ±åˆ (5åˆ†)
1. ç°¡å˜ãªçµ±è¨ˆåé›†æ©Ÿèƒ½
2. `StreamPerformanceMonitor` çµ±åˆ
3. åŸºæœ¬ãƒ†ã‚¹ãƒˆå®Ÿè£…

## ãƒ†ã‚¹ãƒˆè¦ä»¶ï¼ˆRayonç‰ˆï¼‰

### ä¸¦åˆ—å‡¦ç†æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_parallel_processor_creation() {
        let processor = ParallelProcessor::new().unwrap();
        assert!(processor.thread_pool.current_num_threads() > 0);
    }

    #[tokio::test]
    async fn test_parallel_batch_processing() {
        let processor = ParallelProcessor::new().unwrap();
        
        let requests: Vec<StreamRequest> = (0..100)
            .map(|i| create_test_stream_request(i))
            .collect();

        let results = processor.execute_parallel_batch(requests).await;
        assert!(results.is_ok());
        assert_eq!(results.unwrap().len(), 100);
    }

    #[tokio::test]
    async fn test_chunked_parallel_processing() {
        let processor = ParallelProcessor::new().unwrap();
        
        let requests: Vec<StreamRequest> = (0..50)
            .map(|i| create_test_stream_request(i))
            .collect();

        let results = processor.execute_chunked_parallel(requests).await;
        assert_eq!(results.len(), 50);
    }

    #[tokio::test]
    async fn test_backpressure_control() {
        let config = ParallelConfig {
            max_pending_tasks: 10,
            ..Default::default()
        };
        let processor = ParallelProcessor::with_config(config).unwrap();
        
        // å¤§é‡ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ãƒ†ã‚¹ãƒˆ
        let requests: Vec<StreamRequest> = (0..1000)
            .map(|i| create_test_stream_request(i))
            .collect();

        let result = processor.execute_parallel_batch(requests).await;
        // ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡ã®å‹•ä½œç¢ºèª
        assert!(result.is_ok() || matches!(result, Err(ProcessingError::BackpressureExceeded)));
    }

    fn create_test_stream_request(id: usize) -> StreamRequest {
        // ãƒ†ã‚¹ãƒˆç”¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
        StreamRequest {
            message: Some(format!("test-message-{}", id).into()),
        }
    }
}
```

## æˆåŠŸåŸºæº–ï¼ˆRayonç‰ˆï¼‰

### æ©Ÿèƒ½åŸºæº–
- Rayonã«ã‚ˆã‚‹è‡ªå‹•ä¸¦åˆ—å‡¦ç†ãŒæ­£å¸¸å‹•ä½œ
- ãƒãƒƒãƒå‡¦ç†åŠ¹ç‡ > 90%ï¼ˆRayonæœ€é©åŒ–ï¼‰
- è»½é‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- ã‚·ãƒ³ãƒ—ãƒ«ãªçµ±è¨ˆåé›†æ©Ÿèƒ½

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–
- ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ > 2000 req/sï¼ˆRayonã«ã‚ˆã‚‹æœ€é©åŒ–ï¼‰
- Work-stealingåŠ¹ç‡ > 80%
- ã‚¿ã‚¹ã‚¯åˆ†æ•£é…å»¶ < 1msï¼ˆRayonå†…è”µæœ€é©åŒ–ï¼‰
- ã‚»ãƒãƒ•ã‚©ã«ã‚ˆã‚‹ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼åˆ¶å¾¡

### é–‹ç™ºåŠ¹ç‡åŸºæº–
- å®Ÿè£…æ™‚é–“å¤§å¹…çŸ­ç¸®ï¼ˆ75åˆ† â†’ 30åˆ†ï¼‰
- ä¿å®ˆæ€§å‘ä¸Šï¼ˆãƒ—ãƒ­ãƒ™ãƒ³ãƒ©ã‚¤ãƒ–ãƒ©ãƒªæ´»ç”¨ï¼‰
- ã‚³ãƒ¼ãƒ‰è¤‡é›‘åº¦å‰Šæ¸›ï¼ˆè‡ªä½œå®Ÿè£… â†’ Rayonæ´»ç”¨ï¼‰

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

Rayonä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ å®Œäº†å¾Œï¼š
1. Task 3.7 Fix 07-F: æœ€é©åŒ–ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼çµ±åˆå®Ÿè£…
2. å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆãƒ†ã‚¹ãƒˆ
3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œï¼ˆRayonåŠ¹æœæ¸¬å®šï¼‰

## é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- Task 3.7 Fix 07-A (åŸºç›¤ã‚¤ãƒ³ãƒ•ãƒ©æ•´å‚™)
- Task 3.7 Fix 07-B (ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ )
- [Rayonå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.rs/rayon/)
- Rustä¸¦è¡Œãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹