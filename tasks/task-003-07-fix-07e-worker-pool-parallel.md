# Task 3.7 Fix 07-E: æ—¢å­˜WorkerPoolä¸¦åˆ—å‡¦ç†æ‹¡å¼µï¼ˆRayonçµ±åˆç‰ˆï¼‰

## æ¦‚è¦
æ—¢å­˜ã®WorkerPoolã‚’æ‹¡å¼µã—ã€Rayonã‚¯ãƒ¬ãƒ¼ãƒˆã‚’æ´»ç”¨ã—ãŸCPUé›†ç´„çš„ä¸¦åˆ—å‡¦ç†æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¾ã™ã€‚æ—¢å­˜ã®éåŒæœŸã‚¿ã‚¹ã‚¯å‡¦ç†æ©Ÿèƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰ã€CPUé›†ç´„çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰å°‚ç”¨ã®ä¸¦åˆ—å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰ã‚’çµ±åˆã™ã‚‹ã“ã¨ã§ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ä¸€è²«æ€§ã‚’ä¿ã¡ã¤ã¤ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã‚’å®Ÿç¾ã—ã¾ã™ã€‚

## å„ªå…ˆåº¦
**ğŸ”´ æœ€é«˜å„ªå…ˆåº¦** - ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã®ä¸­æ ¸ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

## å®Ÿè£…æ™‚é–“è¦‹ç©ã‚‚ã‚Š
**30åˆ†** - é›†ä¸­ä½œæ¥­æ™‚é–“ï¼ˆæ—¢å­˜WorkerPoolæ‹¡å¼µã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚‹åŠ¹ç‡åŒ–ï¼‰

## ä¾å­˜é–¢ä¿‚
- Task 3.7 Fix 07-A (åŸºç›¤ã‚¤ãƒ³ãƒ•ãƒ©æ•´å‚™) å®Œäº†å¿…é ˆ
- Task 3.7 Fix 07-B (ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ) å®Œäº†æ¨å¥¨

## å—ã‘å…¥ã‚ŒåŸºæº–

### ä¸¦åˆ—å‡¦ç†è¦ä»¶
- [ ] æ—¢å­˜WorkerPoolã¸ã®Rayonçµ±åˆ
- [ ] CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯å°‚ç”¨ãƒ¡ã‚½ãƒƒãƒ‰è¿½åŠ 
- [ ] Work-stealingã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹åŠ¹ç‡çš„è² è·åˆ†æ•£
- [ ] Parallel iteratorã«ã‚ˆã‚‹ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–

### æ—¢å­˜æ©Ÿèƒ½äº’æ›æ€§è¦ä»¶
- [ ] éåŒæœŸã‚¿ã‚¹ã‚¯å‡¦ç†æ©Ÿèƒ½ã®å®Œå…¨ä¿æŒ
- [ ] æ—¢å­˜APIã¨ã®å¾Œæ–¹äº’æ›æ€§ç¶­æŒ
- [ ] ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³æ©Ÿèƒ½ç¶™ç¶š
- [ ] çµ±è¨ˆåé›†ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–çµ±åˆ

### å®‰å®šæ€§è¦ä»¶
- [ ] Rayonã®å†…è”µãƒ‘ãƒ‹ãƒƒã‚¯å¾©æ—§æ©Ÿèƒ½æ´»ç”¨
- [ ] ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
- [ ] Rustã®å‹å®‰å…¨æ€§ã«ã‚ˆã‚‹ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯å›é¿
- [ ] è»½é‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶
- [ ] ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ 2000 req/s é”æˆ
- [ ] ã‚¿ã‚¹ã‚¯åˆ†æ•£é…å»¶ < 1ms
- [ ] ãƒ¯ãƒ¼ã‚«ãƒ¼åˆ©ç”¨ç‡ > 80%
- [ ] ãƒãƒƒãƒå‡¦ç†åŠ¹ç‡ > 90%

## æŠ€è¡“çš„è©³ç´°

### WorkerPoolæ‹¡å¼µå®Ÿè£…ï¼ˆRayonçµ±åˆï¼‰

#### server/Cargo.toml ã¸ã®ä¾å­˜è¿½åŠ 
```toml
[dependencies]
rayon = "1.8"
tokio-rayon = "2.1"  # tokioã¨ã®çµ±åˆç”¨
```

#### src/grpc/performance/worker_pool.rs ã¸ã®æ‹¡å¼µ
```rust
//! ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆRayonä¸¦åˆ—å‡¦ç†æ‹¡å¼µï¼‰
//! 
//! éåŒæœŸã‚¿ã‚¹ã‚¯ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«ã«åŠ ãˆã¦ã€CPUé›†ç´„çš„ä¸¦åˆ—å‡¦ç†æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

use futures::future::BoxFuture;
use rayon::prelude::*;
use std::sync::Arc;
use tokio::sync::{mpsc, Mutex, oneshot};
use tracing::{debug, info, instrument};

/// ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«ï¼ˆRayonæ‹¡å¼µç‰ˆï¼‰
pub struct WorkerPool {
    // æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆå¤‰æ›´ãªã—ï¼‰
    sender: mpsc::Sender<BoxFuture<'static, ()>>,
    handles: Vec<tokio::task::JoinHandle<()>>,
}

/// ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ©ãƒ¼
#[derive(Debug, thiserror::Error)]
pub enum ParallelError {
    #[error("Task processing failed: {0}")]
    TaskFailed(String),
    
    #[error("Task was cancelled")]
    TaskCancelled,
    
    #[error("Join error: {0}")]
    JoinError(#[from] tokio::task::JoinError),
}

impl WorkerPool {
    // æ—¢å­˜ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆå¤‰æ›´ãªã—ï¼‰
    pub fn new(worker_count: usize, queue_capacity: usize) -> Self {
        // æ—¢å­˜å®Ÿè£…ã‚’ä¿æŒ
    }

    pub async fn spawn<F>(&self, task: F) -> Result<(), mpsc::error::SendError<BoxFuture<'static, ()>>>
    where
        F: futures::Future<Output = ()> + Send + 'static,
    {
        // æ—¢å­˜å®Ÿè£…ã‚’ä¿æŒ
    }

    // æ–°æ©Ÿèƒ½ï¼šCPUé›†ç´„çš„ä¸¦åˆ—å‡¦ç†
    /// CPUé›†ç´„çš„ãªä½œæ¥­ã‚’ä¸¦åˆ—å®Ÿè¡Œ
    #[instrument(skip(self, work))]
    pub async fn spawn_cpu_work<F, T>(&self, work: F) -> Result<T, ParallelError>
    where
        F: FnOnce() -> T + Send + 'static,
        T: Send + 'static,
    {
        let (tx, rx) = oneshot::channel();
        
        // Rayonã§ä¸¦åˆ—å®Ÿè¡Œã—ã€çµæœã‚’éåŒæœŸã§è¿”ã™
        rayon::spawn(move || {
            let result = work();
            let _ = tx.send(result);
        });
        
        rx.await.map_err(|_| ParallelError::TaskCancelled)
    }
    
    /// ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦åˆ—å‡¦ç†
    #[instrument(skip(self, items, process_fn))]
    pub async fn spawn_parallel_batch<T, F, R>(
        &self, 
        items: Vec<T>, 
        process_fn: F
    ) -> Result<Vec<R>, ParallelError>
    where
        T: Send + 'static,
        F: Fn(T) -> R + Send + Sync + 'static,
        R: Send + 'static,
    {
        let (tx, rx) = oneshot::channel();
        let process_fn = Arc::new(process_fn);
        
        tokio::task::spawn_blocking(move || {
            let results: Vec<R> = items
                .into_par_iter()
                .map(|item| process_fn(item))
                .collect();
            let _ = tx.send(results);
        });
        
        rx.await.map_err(|_| ParallelError::TaskCancelled)
    }
    
    /// ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ä¸¦åˆ—å‡¦ç†
    pub async fn spawn_chunked_parallel<T, F, R>(
        &self,
        items: Vec<T>,
        chunk_size: usize,
        process_fn: F,
    ) -> Result<Vec<R>, ParallelError>
    where
        T: Send + 'static,
        F: Fn(T) -> R + Send + Sync + 'static,
        R: Send + 'static,
    {
        let (tx, rx) = oneshot::channel();
        let process_fn = Arc::new(process_fn);
        
        tokio::task::spawn_blocking(move || {
            let results: Vec<R> = items
                .par_chunks(chunk_size)
                .flat_map(|chunk| {
                    chunk.par_iter()
                        .map(|item| process_fn(item.clone()))
                })
                .collect();
            let _ = tx.send(results);
        });
        
        rx.await.map_err(|_| ParallelError::TaskCancelled)
    }

    // æ—¢å­˜ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆå¤‰æ›´ãªã—ï¼‰
    pub async fn shutdown(mut self) -> Result<(), tokio::task::JoinError> {
        // æ—¢å­˜å®Ÿè£…ã‚’ä¿æŒ
    }

    pub fn worker_count(&self) -> usize {
        // æ—¢å­˜å®Ÿè£…ã‚’ä¿æŒ
    }
}
```

## å®Ÿè£…è¨ˆç”»ï¼ˆWorkerPoolæ‹¡å¼µç‰ˆï¼‰

### Step 1: Rayonä¾å­˜é–¢ä¿‚è¿½åŠ  (5åˆ†)
1. `Cargo.toml` ã«rayonä¾å­˜é–¢ä¿‚è¿½åŠ 
2. æ—¢å­˜WorkerPoolãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¸ã®Rayonã‚¤ãƒ³ãƒãƒ¼ãƒˆè¿½åŠ 

### Step 2: WorkerPoolä¸¦åˆ—å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰è¿½åŠ  (15åˆ†)
1. `spawn_cpu_work()` ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè£… - CPUé›†ç´„çš„å˜ä¸€ã‚¿ã‚¹ã‚¯
2. `spawn_parallel_batch()` ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè£… - ãƒãƒƒãƒä¸¦åˆ—å‡¦ç†
3. `spawn_chunked_parallel()` ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè£… - ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å‡¦ç†
4. `ParallelError` ã‚¨ãƒ©ãƒ¼å‹å®šç¾©

### Step 3: ãƒ†ã‚¹ãƒˆã¨æ¤œè¨¼ (10åˆ†)
1. æ—¢å­˜ãƒ¡ã‚½ãƒƒãƒ‰ã®å›å¸°ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
2. æ–°ä¸¦åˆ—å‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰ã®ãƒ†ã‚¹ãƒˆå®Ÿè£…
3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

## ãƒ†ã‚¹ãƒˆè¦ä»¶ï¼ˆWorkerPoolæ‹¡å¼µç‰ˆï¼‰

### æ—¢å­˜æ©Ÿèƒ½å›å¸°ãƒ†ã‚¹ãƒˆ
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::time::Instant;

    #[tokio::test]
    async fn test_existing_async_functionality() {
        let pool = WorkerPool::new(4, 100);
        
        // æ—¢å­˜ã®éåŒæœŸã‚¿ã‚¹ã‚¯å‡¦ç†ãŒæ­£å¸¸å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        let result = pool.spawn(async {
            // ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯
        }).await;
        
        assert!(result.is_ok());
        assert_eq!(pool.worker_count(), 4);
    }

    #[tokio::test]
    async fn test_cpu_work_parallel_processing() {
        let pool = WorkerPool::new(4, 100);
        
        // CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œ
        let result = pool.spawn_cpu_work(|| {
            // é‡ã„è¨ˆç®—ã‚¿ã‚¹ã‚¯ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            (0..1000000).sum::<u64>()
        }).await;
        
        assert!(result.is_ok());
        assert_eq!(result.unwrap(), 499999500000u64);
    }

    #[tokio::test]
    async fn test_parallel_batch_processing() {
        let pool = WorkerPool::new(4, 100);
        
        let items: Vec<u32> = (0..100).collect();
        let results = pool.spawn_parallel_batch(items, |x| x * 2).await;
        
        assert!(results.is_ok());
        let results = results.unwrap();
        assert_eq!(results.len(), 100);
        assert_eq!(results[0], 0);
        assert_eq!(results[99], 198);
    }

    #[tokio::test]
    async fn test_chunked_parallel_processing() {
        let pool = WorkerPool::new(4, 100);
        
        let items: Vec<u32> = (0..50).collect();
        let results = pool.spawn_chunked_parallel(items, 10, |x| x * x).await;
        
        assert!(results.is_ok());
        let results = results.unwrap();
        assert_eq!(results.len(), 50);
        assert_eq!(results[0], 0);
        assert_eq!(results[49], 2401);
    }

    #[tokio::test]
    async fn test_performance_comparison() {
        let pool = WorkerPool::new(4, 100);
        
        let items: Vec<u32> = (0..10000).collect();
        
        // é€æ¬¡å‡¦ç†ã®æ™‚é–“æ¸¬å®š
        let start = Instant::now();
        let _sequential: Vec<u64> = items.iter().map(|&x| expensive_computation(x)).collect();
        let sequential_time = start.elapsed();
        
        // ä¸¦åˆ—å‡¦ç†ã®æ™‚é–“æ¸¬å®š
        let start = Instant::now();
        let _parallel = pool.spawn_parallel_batch(items, expensive_computation).await.unwrap();
        let parallel_time = start.elapsed();
        
        // ä¸¦åˆ—å‡¦ç†ãŒé«˜é€Ÿã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆç†æƒ³çš„ã«ã¯ï¼‰
        println!("Sequential: {:?}, Parallel: {:?}", sequential_time, parallel_time);
        
        // æœ€ä½é™ã€ä¸¦åˆ—å‡¦ç†ãŒå®Œäº†ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert!(parallel_time < sequential_time * 2); // å¯›å¤§ãªãƒã‚§ãƒƒã‚¯
    }

    fn expensive_computation(x: u32) -> u64 {
        // ç°¡å˜ãªCPUé›†ç´„çš„è¨ˆç®—
        (0..x % 1000).map(|i| i as u64).sum()
    }
}
```

## æˆåŠŸåŸºæº–ï¼ˆWorkerPoolæ‹¡å¼µç‰ˆï¼‰

### çµ±åˆæ€§åŸºæº–
- æ—¢å­˜ã®éåŒæœŸã‚¿ã‚¹ã‚¯å‡¦ç†æ©Ÿèƒ½ã«å½±éŸ¿ãªã—
- å…¨ã¦ã®æ—¢å­˜ãƒ†ã‚¹ãƒˆãŒãƒ‘ã‚¹
- æ—¢å­˜APIã¨ã®å®Œå…¨ãªå¾Œæ–¹äº’æ›æ€§
- ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³æ©Ÿèƒ½ã®ç¶™ç¶š

### ä¸¦åˆ—å‡¦ç†æ©Ÿèƒ½åŸºæº–
- CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š 2-4å€
- ãƒãƒƒãƒå‡¦ç†åŠ¹ç‡ > 85%
- ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å‡¦ç†ã®æ­£å¸¸å‹•ä½œ
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—åŠ  < 20%

### é–‹ç™ºåŠ¹ç‡åŸºæº–
- ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ä¸€è²«æ€§ä¿æŒ
- æ–°ã—ã„APIã®å­¦ç¿’ã‚³ã‚¹ãƒˆæœ€å°åŒ–
- å˜ä¸€ã‚¯ãƒ©ã‚¹ã§ã®çµ±ä¸€ã•ã‚ŒãŸæ¦‚å¿µ
- ã‚³ãƒ¼ãƒ‰é‡è¤‡ã®å›é¿

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

WorkerPoolä¸¦åˆ—å‡¦ç†æ‹¡å¼µå®Œäº†å¾Œï¼š
1. Task 3.7 Fix 07-F: æœ€é©åŒ–ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã§ã®æ‹¡å¼µWorkerPoolçµ±åˆ
2. æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆãƒ†ã‚¹ãƒˆ
3. éåŒæœŸ vs CPUä¸¦åˆ—å‡¦ç†ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

## é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- Task 3.7 Fix 07-A (åŸºç›¤ã‚¤ãƒ³ãƒ•ãƒ©æ•´å‚™)
- Task 3.7 Fix 07-B (ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ )
- [Rayonå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.rs/rayon/)
- Rustä¸¦è¡Œãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹