# Task 3.7 Fix 07: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

## æ¦‚è¦
Task 3.7ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹å®Ÿè£…ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚å‰ã‚¿ã‚¹ã‚¯ã§å®Ÿæ–½ã—ãŸãƒ†ã‚¹ãƒˆã®çµæœã‚’åŸºã«ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®å‘ä¸Šã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã®å‰Šæ¸›ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

## å„ªå…ˆåº¦
**ğŸŸ¡ ä¸­å„ªå…ˆåº¦** - ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½å‘ä¸Šã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã«å½±éŸ¿

## å®Ÿè£…æ™‚é–“è¦‹ç©ã‚‚ã‚Š
**3-4æ™‚é–“** - é›†ä¸­ä½œæ¥­æ™‚é–“

## å—ã‘å…¥ã‚ŒåŸºæº–

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶
- [ ] ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š: 1000 req/s â†’ 2000 req/s
- [ ] ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼å‰Šæ¸›: P95 50ms â†’ 25msä»¥ä¸‹
- [ ] ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›: ç¾çŠ¶ã‹ã‚‰30%å‰Šæ¸›
- [ ] CPUã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰æœ€å°åŒ–

### ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£è¦ä»¶
- [ ] åŒæ™‚æ¥ç¶šæ•°: 100æ¥ç¶šå¯¾å¿œ
- [ ] ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼æ©Ÿèƒ½ã®å®Ÿè£…
- [ ] å‹•çš„ãƒªã‚½ãƒ¼ã‚¹èª¿æ•´æ©Ÿèƒ½
- [ ] åŠ¹ç‡çš„ãªãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ç®¡ç†

### å®‰å®šæ€§è¦ä»¶
- [ ] é«˜è² è·çŠ¶æ…‹ã§ã®ã‚·ã‚¹ãƒ†ãƒ å®‰å®šæ€§
- [ ] ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ãƒ‡ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
- [ ] ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯å®Œå…¨é˜²æ­¢

## æŠ€è¡“çš„è©³ç´°

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

#### 1. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æœ€é©åŒ–
```rust
/// High-performance message processing pipeline
pub struct OptimizedStreamProcessor {
    // å°‚ç”¨ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«
    worker_pool: Arc<WorkerPool>,
    
    // åŠ¹ç‡çš„ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚­ãƒ¥ãƒ¼
    message_queue: Arc<BoundedMpscQueue<ProcessingTask>>,
    
    // ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ç®¡ç†
    resource_pool: Arc<ResourcePool>,
    
    // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
    performance_monitor: Arc<StreamPerformanceMonitor>,
    
    // è¨­å®šå¯èƒ½ãªæœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    config: OptimizationConfig,
}

#[derive(Debug, Clone)]
pub struct OptimizationConfig {
    // ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆCPUã‚³ã‚¢æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
    pub worker_count: usize,
    
    // ãƒãƒƒãƒå‡¦ç†ã‚µã‚¤ã‚º
    pub batch_size: usize,
    
    // ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚º
    pub queue_capacity: usize,
    
    // ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼é–¾å€¤
    pub backpressure_threshold: f64,
    
    // ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã‚µã‚¤ã‚º
    pub prefetch_size: usize,
    
    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º
    pub cache_size: usize,
}

impl Default for OptimizationConfig {
    fn default() -> Self {
        let cpu_count = num_cpus::get();
        Self {
            worker_count: cpu_count.max(4),
            batch_size: 10,
            queue_capacity: 10000,
            backpressure_threshold: 0.8,
            prefetch_size: 100,
            cache_size: 1000,
        }
    }
}

impl OptimizedStreamProcessor {
    pub fn new(config: OptimizationConfig) -> Self {
        let worker_pool = Arc::new(WorkerPool::new(config.worker_count));
        let message_queue = Arc::new(BoundedMpscQueue::new(config.queue_capacity));
        let resource_pool = Arc::new(ResourcePool::new());
        let performance_monitor = Arc::new(StreamPerformanceMonitor::new());
        
        Self {
            worker_pool,
            message_queue,
            resource_pool,
            performance_monitor,
            config,
        }
    }

    pub async fn process_stream_optimized(
        &self,
        mut incoming_stream: Streaming<StreamRequest>,
        response_sender: tokio::sync::mpsc::Sender<Result<StreamResponse, Status>>,
    ) {
        let start_time = std::time::Instant::now();
        let connection_id = uuid::Uuid::new_v4().to_string();
        
        info!(connection_id = %connection_id, "Starting optimized stream processing");

        // ãƒãƒƒãƒå‡¦ç†ç”¨ãƒãƒƒãƒ•ã‚¡
        let mut batch_buffer = Vec::with_capacity(self.config.batch_size);
        let mut message_counter = 0u64;

        while let Some(result) = incoming_stream.next().await {
            match result {
                Ok(stream_request) => {
                    message_counter += 1;
                    
                    let processing_task = ProcessingTask {
                        request: stream_request,
                        message_id: message_counter,
                        connection_id: connection_id.clone(),
                        timestamp: std::time::Instant::now(),
                    };

                    batch_buffer.push(processing_task);

                    // ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰å‡¦ç†
                    if batch_buffer.len() >= self.config.batch_size {
                        self.process_batch(
                            std::mem::take(&mut batch_buffer),
                            &response_sender,
                        ).await;
                    }

                    // ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ãƒã‚§ãƒƒã‚¯
                    if self.should_apply_backpressure().await {
                        self.handle_backpressure(&response_sender).await;
                    }
                }
                Err(status) => {
                    warn!(connection_id = %connection_id, error = %status, "Stream error encountered");
                    
                    // æ®‹ã‚Šã®ãƒãƒƒãƒã‚’å‡¦ç†ã—ã¦ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’é€ä¿¡
                    if !batch_buffer.is_empty() {
                        self.process_batch(batch_buffer, &response_sender).await;
                    }
                    
                    let error_response = self.create_stream_error_response(status);
                    let _ = response_sender.send(Ok(error_response)).await;
                    break;
                }
            }
        }

        // æ®‹ã‚Šã®ãƒãƒƒãƒ•ã‚¡ã‚’å‡¦ç†
        if !batch_buffer.is_empty() {
            self.process_batch(batch_buffer, &response_sender).await;
        }

        let total_time = start_time.elapsed();
        self.performance_monitor.record_stream_session(
            connection_id.clone(),
            message_counter,
            total_time,
        );

        info!(
            connection_id = %connection_id,
            message_count = message_counter,
            duration = ?total_time,
            "Optimized stream processing completed"
        );
    }

    async fn process_batch(
        &self,
        batch: Vec<ProcessingTask>,
        response_sender: &tokio::sync::mpsc::Sender<Result<StreamResponse, Status>>,
    ) {
        let batch_start = std::time::Instant::now();
        let batch_size = batch.len();

        // ä¸¦åˆ—å‡¦ç†ç”¨ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
        let mut worker_tasks = Vec::new();
        
        for task in batch {
            let worker = Arc::clone(&self.worker_pool);
            let resource_pool = Arc::clone(&self.resource_pool);
            
            let worker_task = tokio::spawn(async move {
                worker.execute(task, resource_pool).await
            });
            
            worker_tasks.push(worker_task);
        }

        // å…¨ã¦ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¹ã‚¯ã®å®Œäº†ã‚’å¾…æ©Ÿ
        let results = futures::future::join_all(worker_tasks).await;

        // çµæœã‚’ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒãƒ£ãƒãƒ«ã«é€ä¿¡
        for result in results {
            match result {
                Ok(Ok(response)) => {
                    if response_sender.send(Ok(response)).await.is_err() {
                        warn!("Failed to send response - receiver dropped");
                        break;
                    }
                }
                Ok(Err(error_response)) => {
                    if response_sender.send(Ok(error_response)).await.is_err() {
                        warn!("Failed to send error response - receiver dropped");
                        break;
                    }
                }
                Err(join_error) => {
                    warn!(error = %join_error, "Worker task failed");
                    // é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆã—ã¦é€ä¿¡
                    let error_response = self.create_worker_error_response(join_error);
                    let _ = response_sender.send(Ok(error_response)).await;
                }
            }
        }

        let batch_duration = batch_start.elapsed();
        self.performance_monitor.record_batch_processing(batch_size, batch_duration);
        
        debug!(
            batch_size = batch_size,
            duration = ?batch_duration,
            throughput = batch_size as f64 / batch_duration.as_secs_f64(),
            "Batch processing completed"
        );
    }

    async fn should_apply_backpressure(&self) -> bool {
        let queue_usage = self.message_queue.usage_ratio().await;
        let worker_usage = self.worker_pool.usage_ratio().await;
        
        queue_usage > self.config.backpressure_threshold ||
        worker_usage > self.config.backpressure_threshold
    }

    async fn handle_backpressure(
        &self,
        response_sender: &tokio::sync::mpsc::Sender<Result<StreamResponse, Status>>,
    ) {
        warn!("Applying backpressure due to high load");
        
        // ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼è­¦å‘Šã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é€ä¿¡
        let backpressure_response = self.create_backpressure_warning();
        let _ = response_sender.send(Ok(backpressure_response)).await;
        
        // çŸ­æ™‚é–“å¾…æ©Ÿã—ã¦ã‚·ã‚¹ãƒ†ãƒ è² è·ã‚’è»½æ¸›
        tokio::time::sleep(std::time::Duration::from_millis(10)).await;
        
        self.performance_monitor.record_backpressure_event();
    }
}
```

#### 2. åŠ¹ç‡çš„ãªãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«
```rust
/// High-performance worker pool for message processing
pub struct WorkerPool {
    workers: Vec<Worker>,
    task_scheduler: Arc<TaskScheduler>,
    load_balancer: Arc<LoadBalancer>,
}

pub struct Worker {
    id: usize,
    task_queue: tokio::sync::mpsc::Receiver<ProcessingTask>,
    service_instance: Arc<UnityMcpServiceImpl>,
    performance_counter: Arc<WorkerPerformanceCounter>,
}

impl WorkerPool {
    pub fn new(worker_count: usize) -> Self {
        let task_scheduler = Arc::new(TaskScheduler::new());
        let load_balancer = Arc::new(LoadBalancer::new(worker_count));
        
        let workers = (0..worker_count)
            .map(|id| Worker::new(id, Arc::clone(&task_scheduler)))
            .collect();

        Self {
            workers,
            task_scheduler,
            load_balancer,
        }
    }

    pub async fn execute(
        &self,
        task: ProcessingTask,
        resource_pool: Arc<ResourcePool>,
    ) -> Result<StreamResponse, StreamResponse> {
        let worker_id = self.load_balancer.select_worker(&task).await;
        
        // ã‚¿ã‚¹ã‚¯ã‚’é¸æŠã•ã‚ŒãŸãƒ¯ãƒ¼ã‚«ãƒ¼ã«é€ä¿¡
        self.task_scheduler.schedule_task(worker_id, task).await
            .map_err(|e| self.create_scheduling_error(e))
    }

    pub async fn usage_ratio(&self) -> f64 {
        let total_capacity = self.workers.len() as f64;
        let busy_workers = self.count_busy_workers().await as f64;
        busy_workers / total_capacity
    }

    async fn count_busy_workers(&self) -> usize {
        // å®Ÿè£…: ãƒ“ã‚¸ãƒ¼ãªãƒ¯ãƒ¼ã‚«ãƒ¼ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        0 // ã‚¹ã‚¿ãƒ–
    }
}

impl Worker {
    pub fn new(id: usize, task_scheduler: Arc<TaskScheduler>) -> Self {
        let (task_sender, task_receiver) = tokio::sync::mpsc::channel(1000);
        let service_instance = Arc::new(UnityMcpServiceImpl::new());
        let performance_counter = Arc::new(WorkerPerformanceCounter::new());

        // ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
        let worker_task = Self::start_worker_loop(
            id,
            task_receiver,
            Arc::clone(&service_instance),
            Arc::clone(&performance_counter),
        );
        
        tokio::spawn(worker_task);

        // ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã«ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ç™»éŒ²
        task_scheduler.register_worker(id, task_sender);

        Self {
            id,
            task_queue: task_receiver,
            service_instance,
            performance_counter,
        }
    }

    async fn start_worker_loop(
        worker_id: usize,
        mut task_queue: tokio::sync::mpsc::Receiver<ProcessingTask>,
        service: Arc<UnityMcpServiceImpl>,
        performance_counter: Arc<WorkerPerformanceCounter>,
    ) {
        info!(worker_id = worker_id, "Worker started");

        while let Some(task) = task_queue.recv().await {
            let task_start = std::time::Instant::now();
            
            let response = Self::process_single_task(&service, task).await;
            
            let task_duration = task_start.elapsed();
            performance_counter.record_task(task_duration, response.is_ok());
        }

        info!(worker_id = worker_id, "Worker terminated");
    }

    async fn process_single_task(
        service: &Arc<UnityMcpServiceImpl>,
        task: ProcessingTask,
    ) -> Result<StreamResponse, StreamResponse> {
        match task.request.message {
            Some(stream_request::Message::ImportAsset(req)) => {
                ImportAssetStreamHandler::handle(
                    service,
                    req,
                    task.connection_id,
                    task.message_id,
                ).await
            }
            Some(stream_request::Message::MoveAsset(req)) => {
                MoveAssetStreamHandler::handle(
                    service,
                    req,
                    task.connection_id,
                    task.message_id,
                ).await
            }
            Some(stream_request::Message::DeleteAsset(req)) => {
                DeleteAssetStreamHandler::handle(
                    service,
                    req,
                    task.connection_id,
                    task.message_id,
                ).await
            }
            Some(stream_request::Message::Refresh(req)) => {
                RefreshStreamHandler::handle(
                    service,
                    req,
                    task.connection_id,
                    task.message_id,
                ).await
            }
            None => Err(Self::create_empty_message_error(task.message_id)),
        }
        .map_err(|error_response| error_response) // ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚‚Okæ‰±ã„ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ ã¯ç¶™ç¶šï¼‰
    }
}
```

#### 3. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
```rust
/// Intelligent caching system for stream processing
pub struct StreamCache {
    // LRU ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    response_cache: Arc<Mutex<lru::LruCache<CacheKey, CacheEntry>>>,
    
    // çµ±è¨ˆæƒ…å ±
    cache_stats: Arc<CacheStatistics>,
    
    // è¨­å®š
    config: CacheConfig,
}

#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct CacheKey {
    operation_type: String,
    request_hash: u64,
}

#[derive(Debug, Clone)]
pub struct CacheEntry {
    response: StreamResponse,
    created_at: std::time::Instant,
    access_count: u64,
}

#[derive(Debug, Clone)]
pub struct CacheConfig {
    pub max_entries: usize,
    pub ttl: std::time::Duration,
    pub enable_prefetch: bool,
}

impl StreamCache {
    pub fn new(config: CacheConfig) -> Self {
        Self {
            response_cache: Arc::new(Mutex::new(
                lru::LruCache::new(std::num::NonZeroUsize::new(config.max_entries).unwrap())
            )),
            cache_stats: Arc::new(CacheStatistics::new()),
            config,
        }
    }

    pub async fn get(&self, key: &CacheKey) -> Option<StreamResponse> {
        if let Ok(mut cache) = self.response_cache.lock() {
            if let Some(entry) = cache.get_mut(key) {
                // TTL ãƒã‚§ãƒƒã‚¯
                if entry.created_at.elapsed() < self.config.ttl {
                    entry.access_count += 1;
                    self.cache_stats.record_hit();
                    return Some(entry.response.clone());
                } else {
                    // æœŸé™åˆ‡ã‚Œã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤
                    cache.pop(key);
                }
            }
        }
        
        self.cache_stats.record_miss();
        None
    }

    pub async fn put(&self, key: CacheKey, response: StreamResponse) {
        if let Ok(mut cache) = self.response_cache.lock() {
            let entry = CacheEntry {
                response,
                created_at: std::time::Instant::now(),
                access_count: 1,
            };
            
            cache.put(key, entry);
            self.cache_stats.record_store();
        }
    }

    pub fn generate_cache_key(&self, request: &StreamRequest) -> Option<CacheKey> {
        match &request.message {
            Some(stream_request::Message::ImportAsset(req)) => {
                Some(CacheKey {
                    operation_type: "import_asset".to_string(),
                    request_hash: self.hash_import_request(req),
                })
            }
            Some(stream_request::Message::MoveAsset(req)) => {
                Some(CacheKey {
                    operation_type: "move_asset".to_string(),
                    request_hash: self.hash_move_request(req),
                })
            }
            // Deleteã¨Refreshã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ãªã„ï¼ˆå‰¯ä½œç”¨ãŒã‚ã‚‹ãŸã‚ï¼‰
            _ => None,
        }
    }

    fn hash_import_request(&self, req: &ImportAssetRequest) -> u64 {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        req.asset_path.hash(&mut hasher);
        hasher.finish()
    }

    fn hash_move_request(&self, req: &MoveAssetRequest) -> u64 {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};
        
        let mut hasher = DefaultHasher::new();
        req.src_path.hash(&mut hasher);
        req.dst_path.hash(&mut hasher);
        hasher.finish()
    }

    pub fn get_cache_statistics(&self) -> CacheStatistics {
        self.cache_stats.clone()
    }
}
```

#### 4. ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ç®¡ç†
```rust
/// Efficient resource pool management
pub struct ResourcePool {
    // ã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ—ãƒ¼ãƒ«
    service_pool: Arc<ObjectPool<UnityMcpServiceImpl>>,
    
    // ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ³ã‚¸ãƒ³ãƒ—ãƒ¼ãƒ«
    validator_pool: Arc<ObjectPool<StreamValidationEngine>>,
    
    // ä¸€æ™‚çš„ãªãƒãƒƒãƒ•ã‚¡ãƒ—ãƒ¼ãƒ«
    buffer_pool: Arc<ObjectPool<Vec<u8>>>,
    
    // ãƒ—ãƒ¼ãƒ«çµ±è¨ˆ
    pool_stats: Arc<PoolStatistics>,
}

pub struct ObjectPool<T> {
    objects: Arc<Mutex<Vec<T>>>,
    factory: Arc<dyn Fn() -> T + Send + Sync>,
    max_size: usize,
}

impl<T> ObjectPool<T> {
    pub fn new<F>(factory: F, max_size: usize) -> Self
    where
        F: Fn() -> T + Send + Sync + 'static,
    {
        Self {
            objects: Arc::new(Mutex::new(Vec::with_capacity(max_size))),
            factory: Arc::new(factory),
            max_size,
        }
    }

    pub async fn get(&self) -> PooledObject<T> {
        if let Ok(mut objects) = self.objects.lock() {
            if let Some(object) = objects.pop() {
                return PooledObject::new(object, Arc::clone(&self.objects));
            }
        }
        
        // ãƒ—ãƒ¼ãƒ«ãŒç©ºã®å ´åˆã¯æ–°ã—ã„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        let object = (self.factory)();
        PooledObject::new(object, Arc::clone(&self.objects))
    }
}

pub struct PooledObject<T> {
    object: Option<T>,
    pool: Arc<Mutex<Vec<T>>>,
}

impl<T> PooledObject<T> {
    fn new(object: T, pool: Arc<Mutex<Vec<T>>>) -> Self {
        Self {
            object: Some(object),
            pool,
        }
    }
}

impl<T> std::ops::Deref for PooledObject<T> {
    type Target = T;

    fn deref(&self) -> &Self::Target {
        self.object.as_ref().unwrap()
    }
}

impl<T> std::ops::DerefMut for PooledObject<T> {
    fn deref_mut(&mut self) -> &mut Self::Target {
        self.object.as_mut().unwrap()
    }
}

impl<T> Drop for PooledObject<T> {
    fn drop(&mut self) {
        if let Some(object) = self.object.take() {
            if let Ok(mut pool) = self.pool.lock() {
                pool.push(object);
            }
        }
    }
}

impl ResourcePool {
    pub fn new() -> Self {
        Self {
            service_pool: Arc::new(ObjectPool::new(
                || UnityMcpServiceImpl::new(),
                10,
            )),
            validator_pool: Arc::new(ObjectPool::new(
                || StreamValidationEngine::new(),
                5,
            )),
            buffer_pool: Arc::new(ObjectPool::new(
                || Vec::with_capacity(8192),
                50,
            )),
            pool_stats: Arc::new(PoolStatistics::new()),
        }
    }

    pub async fn get_service(&self) -> PooledObject<UnityMcpServiceImpl> {
        let service = self.service_pool.get().await;
        self.pool_stats.record_service_acquisition();
        service
    }

    pub async fn get_validator(&self) -> PooledObject<StreamValidationEngine> {
        let validator = self.validator_pool.get().await;
        self.pool_stats.record_validator_acquisition();
        validator
    }

    pub async fn get_buffer(&self) -> PooledObject<Vec<u8>> {
        let mut buffer = self.buffer_pool.get().await;
        buffer.clear(); // ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
        self.pool_stats.record_buffer_acquisition();
        buffer
    }
}
```

#### 5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
```rust
/// Comprehensive performance monitoring system
pub struct StreamPerformanceMonitor {
    // ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
    metrics: Arc<Mutex<PerformanceMetrics>>,
    
    // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆ
    real_time_stats: Arc<RealTimeStats>,
    
    // å±¥æ­´ãƒ‡ãƒ¼ã‚¿
    historical_data: Arc<Mutex<HistoricalData>>,
}

#[derive(Debug, Default)]
pub struct PerformanceMetrics {
    // ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
    pub total_requests: u64,
    pub successful_requests: u64,
    pub failed_requests: u64,
    
    // ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼
    pub latency_sum: std::time::Duration,
    pub latency_min: std::time::Duration,
    pub latency_max: std::time::Duration,
    
    // ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
    pub current_memory_usage: usize,
    pub peak_memory_usage: usize,
    
    // ãƒ¯ãƒ¼ã‚«ãƒ¼çµ±è¨ˆ
    pub worker_utilization: f64,
    pub queue_depth: usize,
    
    // ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
    pub cache_hit_ratio: f64,
}

impl StreamPerformanceMonitor {
    pub fn new() -> Self {
        Self {
            metrics: Arc::new(Mutex::new(PerformanceMetrics::default())),
            real_time_stats: Arc::new(RealTimeStats::new()),
            historical_data: Arc::new(Mutex::new(HistoricalData::new())),
        }
    }

    pub fn record_stream_session(
        &self,
        connection_id: String,
        message_count: u64,
        duration: std::time::Duration,
    ) {
        if let Ok(mut metrics) = self.metrics.lock() {
            metrics.total_requests += message_count;
            metrics.successful_requests += message_count; // ç°¡ç•¥åŒ–
            
            let avg_latency = duration / message_count as u32;
            metrics.latency_sum += duration;
            metrics.latency_min = metrics.latency_min.min(avg_latency);
            metrics.latency_max = metrics.latency_max.max(avg_latency);
        }

        self.real_time_stats.update_throughput(message_count, duration);
        
        if let Ok(mut history) = self.historical_data.lock() {
            history.record_session(connection_id, message_count, duration);
        }
    }

    pub fn record_batch_processing(&self, batch_size: usize, duration: std::time::Duration) {
        let throughput = batch_size as f64 / duration.as_secs_f64();
        self.real_time_stats.update_batch_throughput(throughput);
    }

    pub fn record_backpressure_event(&self) {
        self.real_time_stats.increment_backpressure_events();
    }

    pub fn get_current_metrics(&self) -> PerformanceMetrics {
        if let Ok(metrics) = self.metrics.lock() {
            metrics.clone()
        } else {
            PerformanceMetrics::default()
        }
    }

    pub fn get_performance_summary(&self) -> PerformanceSummary {
        let metrics = self.get_current_metrics();
        let real_time = self.real_time_stats.get_current_stats();
        
        PerformanceSummary {
            throughput: real_time.current_throughput,
            avg_latency: if metrics.total_requests > 0 {
                metrics.latency_sum / metrics.total_requests as u32
            } else {
                std::time::Duration::default()
            },
            success_rate: if metrics.total_requests > 0 {
                metrics.successful_requests as f64 / metrics.total_requests as f64
            } else {
                0.0
            },
            memory_usage: metrics.current_memory_usage,
            worker_utilization: metrics.worker_utilization,
            cache_efficiency: metrics.cache_hit_ratio,
        }
    }
}

#[derive(Debug, Clone)]
pub struct PerformanceSummary {
    pub throughput: f64,              // req/s
    pub avg_latency: std::time::Duration,
    pub success_rate: f64,            // 0.0-1.0
    pub memory_usage: usize,          // bytes
    pub worker_utilization: f64,      // 0.0-1.0
    pub cache_efficiency: f64,        // 0.0-1.0
}
```

#### 6. çµ±åˆã•ã‚ŒãŸæœ€é©åŒ–ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†
```rust
/// Optimized stream service implementation
impl UnityMcpService for UnityMcpServiceImpl {
    #[instrument(skip(self))]
    async fn stream(
        &self,
        request: Request<Streaming<StreamRequest>>,
    ) -> Result<Response<Self::StreamStream>, Status> {
        info!("Optimized stream connection established");

        // æœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’ä½œæˆ
        let optimization_config = OptimizationConfig::default();
        let processor = OptimizedStreamProcessor::new(optimization_config);
        
        // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹
        let incoming_stream = request.into_inner();
        let (response_sender, response_receiver) = tokio::sync::mpsc::channel(10000);
        
        // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
        tokio::spawn(async move {
            processor.process_stream_optimized(incoming_stream, response_sender).await;
        });

        // ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ä½œæˆ
        let response_stream = tokio_stream::wrappers::ReceiverStream::new(response_receiver);
        let boxed_stream: Self::StreamStream = Box::pin(response_stream);

        Ok(Response::new(boxed_stream))
    }
}
```

## å®Ÿè£…è¨ˆç”»

### Step 1: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºç›¤ã®æ§‹ç¯‰
1. `OptimizedStreamProcessor`ã®å®Ÿè£…
2. è¨­å®šå¯èƒ½ãªæœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®šç¾©
3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰

### Step 2: ä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…
1. åŠ¹ç‡çš„ãªãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«ã®å®Ÿè£…
2. ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã¨ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°
3. ãƒãƒƒãƒå‡¦ç†æ©Ÿèƒ½ã®å®Ÿè£…

### Step 3: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…
1. LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å®Ÿè£…
2. ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªã‚­ãƒ¼ãƒ’ãƒ³ã‚°æˆ¦ç•¥
3. TTLç®¡ç†ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ

### Step 4: ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†æœ€é©åŒ–
1. ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ—ãƒ¼ãƒ«ã®å®Ÿè£…
2. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒãƒƒãƒ•ã‚¡ç®¡ç†
3. ãƒªã‚½ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†

### Step 5: çµ±åˆãƒ†ã‚¹ãƒˆã¨èª¿æ•´
1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
2. æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
3. è² è·ãƒ†ã‚¹ãƒˆã¨å®‰å®šæ€§ç¢ºèª

## ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨æ¤œè¨¼

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
```rust
#[cfg(test)]
mod performance_benchmarks {
    use super::*;
    
    #[tokio::test]
    async fn benchmark_optimized_throughput() {
        let processor = OptimizedStreamProcessor::new(OptimizationConfig::default());
        
        let request_count = 20000;
        let start_time = std::time::Instant::now();
        
        // é«˜è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        let results = run_throughput_test(&processor, request_count).await;
        
        let elapsed = start_time.elapsed();
        let throughput = request_count as f64 / elapsed.as_secs_f64();
        
        println!("Optimized throughput: {:.2} req/s", throughput);
        assert!(throughput > 2000.0, "Target throughput not achieved");
    }

    #[tokio::test]
    async fn benchmark_memory_efficiency() {
        let processor = OptimizedStreamProcessor::new(OptimizationConfig::default());
        
        let initial_memory = get_current_memory_usage();
        
        // ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ
        run_memory_test(&processor).await;
        
        let peak_memory = get_peak_memory_usage();
        let memory_increase = peak_memory - initial_memory;
        
        println!("Memory increase: {} MB", memory_increase / 1024 / 1024);
        assert!(memory_increase < 50 * 1024 * 1024, "Memory usage too high");
    }

    #[tokio::test]
    async fn benchmark_latency_optimization() {
        let processor = OptimizedStreamProcessor::new(OptimizationConfig::default());
        
        let latency_results = run_latency_test(&processor).await;
        
        println!("P95 latency: {:?}", latency_results.p95);
        println!("P99 latency: {:?}", latency_results.p99);
        
        assert!(latency_results.p95 < std::time::Duration::from_millis(25));
        assert!(latency_results.p99 < std::time::Duration::from_millis(50));
    }
}
```

## æˆåŠŸåŸºæº–

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
- **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: 2000 req/sä»¥ä¸Š
- **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼**: P95 25msä»¥ä¸‹ã€P99 50msä»¥ä¸‹
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: ç¾çŠ¶ã‹ã‚‰30%å‰Šæ¸›
- **CPUä½¿ç”¨ç‡**: åŠ¹ç‡çš„ãªåˆ©ç”¨ï¼ˆ80%ä»¥ä¸‹ï¼‰

### ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æŒ‡æ¨™
- **åŒæ™‚æ¥ç¶š**: 100æ¥ç¶šã®å®‰å®šå‡¦ç†
- **è² è·è€æ€§**: é«˜è² è·çŠ¶æ…‹ã§ã®å®‰å®šå‹•ä½œ
- **ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡**: ãƒ—ãƒ¼ãƒ«åˆ©ç”¨ç‡ã®æœ€é©åŒ–

### å“è³ªæŒ‡æ¨™
- **ã‚¨ãƒ©ãƒ¼ç‡**: 1%ä»¥ä¸‹ç¶­æŒ
- **ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯**: å®Œå…¨é˜²æ­¢
- **ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¸€è²«æ€§**: 100%

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

æœ€é©åŒ–å®Œäº†å¾Œ:
1. æœ¬ç•ªç’°å¢ƒã§ã®æ®µéšçš„ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆ
2. ç¶™ç¶šçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
3. æœ€é©åŒ–åŠ¹æœã®é•·æœŸè©•ä¾¡

## é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- Task 3.7 Fix 06 (åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ)
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
- æœ€é©åŒ–è¨­å®šã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³