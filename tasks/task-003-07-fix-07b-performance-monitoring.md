# Task 3.7 Fix 07-B: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

## æ¦‚è¦
ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…ã—ã¾ã™ã€‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã€çµ±è¨ˆåˆ†æã€å±¥æ­´ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚’é€šã˜ã¦ã€æœ€é©åŒ–ã®åŠ¹æœæ¸¬å®šã¨ç¶™ç¶šçš„æ”¹å–„ã®ãŸã‚ã®åŸºç›¤ã‚’æä¾›ã—ã¾ã™ã€‚

## å„ªå…ˆåº¦
**ğŸŸ¡ é«˜å„ªå…ˆåº¦** - æœ€é©åŒ–åŠ¹æœã®æ¸¬å®šã¨ãƒ‡ãƒãƒƒã‚°ã«å¿…é ˆ

## å®Ÿè£…æ™‚é–“è¦‹ç©ã‚‚ã‚Š
**60åˆ†** - é›†ä¸­ä½œæ¥­æ™‚é–“

## ä¾å­˜é–¢ä¿‚
- Task 3.7 Fix 07-A (åŸºç›¤ã‚¤ãƒ³ãƒ•ãƒ©æ•´å‚™) å®Œäº†å¿…é ˆ

## å—ã‘å…¥ã‚ŒåŸºæº–

### ç›£è¦–æ©Ÿèƒ½è¦ä»¶
- [ ] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
- [ ] ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¸¬å®š
- [ ] åŒæ™‚æ¥ç¶šæ•°ã¨ãƒ¯ãƒ¼ã‚«ãƒ¼åˆ©ç”¨ç‡ã®è¿½è·¡
- [ ] çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®å±¥æ­´ä¿æŒ

### ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†è¦ä»¶
- [ ] P95/P99ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼è¨ˆç®—
- [ ] æ¯ç§’ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ï¼ˆRPSï¼‰æ¸¬å®š
- [ ] ã‚¨ãƒ©ãƒ¼ç‡ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã®è¨ˆç®—
- [ ] ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ç™ºç”Ÿå›æ•°ã®è¨˜éŒ²

### çµ±è¨ˆåˆ†æè¦ä»¶
- [ ] ç§»å‹•å¹³å‡ã¨ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç•°å¸¸ã®æ¤œå‡º
- [ ] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆã®æ›´æ–°
- [ ] å®šæœŸçš„ãªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

### ç›£è¦–ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›è¦ä»¶
- [ ] æ§‹é€ åŒ–ãƒ­ã‚°å‡ºåŠ›
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ã®æä¾›
- [ ] ãƒ‡ãƒãƒƒã‚°ç”¨è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
- [ ] Prometheusäº’æ›ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

## æŠ€è¡“çš„è©³ç´°

### StreamPerformanceMonitor å®Ÿè£…

#### src/grpc/performance/monitor.rs
```rust
//! ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
//! 
//! ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©³ç´°ã«ç›£è¦–ã—ã€
//! æœ€é©åŒ–ã®åŠ¹æœæ¸¬å®šã¨ãƒ‡ãƒãƒƒã‚°ã®ãŸã‚ã®æƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚

use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use std::collections::VecDeque;
use tokio::time::interval;
use tracing::{info, debug, warn};
use serde::{Serialize, Deserialize};

/// åŒ…æ‹¬çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
pub struct StreamPerformanceMonitor {
    // ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    current_metrics: Arc<Mutex<PerformanceMetrics>>,
    
    // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆ
    real_time_stats: Arc<Mutex<RealTimeStats>>,
    
    // å±¥æ­´ãƒ‡ãƒ¼ã‚¿ï¼ˆç’°çŠ¶ãƒãƒƒãƒ•ã‚¡ï¼‰
    historical_data: Arc<Mutex<HistoricalData>>,
    
    // ã‚»ãƒƒã‚·ãƒ§ãƒ³è¿½è·¡
    active_sessions: Arc<Mutex<SessionTracker>>,
    
    // è¨­å®š
    monitoring_config: MonitoringConfig,
}

/// ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
#[derive(Debug, Default, Clone, Serialize, Deserialize)]
pub struct PerformanceMetrics {
    // åŸºæœ¬çµ±è¨ˆ
    pub total_requests: u64,
    pub successful_requests: u64,
    pub failed_requests: u64,
    pub active_connections: u64,
    
    // ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼çµ±è¨ˆ
    pub latency_sum_ms: f64,
    pub latency_min_ms: f64,
    pub latency_max_ms: f64,
    pub latency_p95_ms: f64,
    pub latency_p99_ms: f64,
    
    // ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆçµ±è¨ˆ
    pub current_rps: f64,
    pub peak_rps: f64,
    pub avg_rps: f64,
    
    // ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡
    pub memory_usage_bytes: usize,
    pub peak_memory_bytes: usize,
    pub cpu_usage_percent: f64,
    
    // ã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§
    pub worker_utilization: f64,
    pub queue_depth: usize,
    pub cache_hit_ratio: f64,
    pub backpressure_events: u64,
    
    // ã‚¿ã‚¤ãƒŸãƒ³ã‚°
    pub monitoring_start: Option<Instant>,
    pub last_update: Option<Instant>,
}

/// ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆå‡¦ç†
#[derive(Debug)]
pub struct RealTimeStats {
    // ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—ç”¨
    request_timestamps: VecDeque<Instant>,
    throughput_window: Duration,
    
    // ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼è¨ˆç®—ç”¨
    latency_samples: VecDeque<Duration>,
    max_latency_samples: usize,
    
    // ç§»å‹•å¹³å‡
    rps_moving_avg: MovingAverage,
    latency_moving_avg: MovingAverage,
    
    // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç•°å¸¸æ¤œå‡º
    anomaly_detector: AnomalyDetector,
}

/// å±¥æ­´ãƒ‡ãƒ¼ã‚¿ç®¡ç†
#[derive(Debug)]
pub struct HistoricalData {
    // ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆï¼ˆç’°çŠ¶ãƒãƒƒãƒ•ã‚¡ï¼‰
    data_points: VecDeque<HistoricalDataPoint>,
    max_history_size: usize,
    
    // é›†ç´„ãƒ‡ãƒ¼ã‚¿
    hourly_aggregates: VecDeque<HourlyAggregate>,
    daily_aggregates: VecDeque<DailyAggregate>,
}

/// å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct HistoricalDataPoint {
    pub timestamp: Instant,
    pub rps: f64,
    pub avg_latency_ms: f64,
    pub memory_mb: f64,
    pub active_connections: u64,
    pub error_rate: f64,
}

/// ã‚»ãƒƒã‚·ãƒ§ãƒ³è¿½è·¡
#[derive(Debug)]
pub struct SessionTracker {
    sessions: std::collections::HashMap<String, SessionInfo>,
    session_stats: SessionStatistics,
}

/// å€‹åˆ¥ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±
#[derive(Debug, Clone)]
pub struct SessionInfo {
    pub connection_id: String,
    pub start_time: Instant,
    pub message_count: u64,
    pub last_activity: Instant,
    pub bytes_processed: usize,
}

/// ç›£è¦–è¨­å®š
#[derive(Debug, Clone)]
pub struct MonitoringConfig {
    pub collection_interval: Duration,
    pub history_retention: Duration,
    pub throughput_window: Duration,
    pub max_latency_samples: usize,
    pub enable_detailed_logging: bool,
    pub anomaly_detection_threshold: f64,
}

impl Default for MonitoringConfig {
    fn default() -> Self {
        Self {
            collection_interval: Duration::from_secs(1),
            history_retention: Duration::from_hours(24),
            throughput_window: Duration::from_secs(10),
            max_latency_samples: 1000,
            enable_detailed_logging: false,
            anomaly_detection_threshold: 2.0, // æ¨™æº–åå·®ã®å€æ•°
        }
    }
}

impl StreamPerformanceMonitor {
    /// æ–°ã—ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    pub fn new() -> Self {
        Self::with_config(MonitoringConfig::default())
    }

    /// è¨­å®šä»˜ãã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    pub fn with_config(config: MonitoringConfig) -> Self {
        let monitor = Self {
            current_metrics: Arc::new(Mutex::new(PerformanceMetrics {
                monitoring_start: Some(Instant::now()),
                latency_min_ms: f64::INFINITY,
                ..Default::default()
            })),
            real_time_stats: Arc::new(Mutex::new(RealTimeStats::new(&config))),
            historical_data: Arc::new(Mutex::new(HistoricalData::new(&config))),
            active_sessions: Arc::new(Mutex::new(SessionTracker::new())),
            monitoring_config: config,
        };

        // å®šæœŸçš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°ã‚’é–‹å§‹
        monitor.start_periodic_updates();
        
        info!("Performance monitoring system initialized");
        monitor
    }

    /// ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è¨˜éŒ²
    pub fn record_stream_session(
        &self,
        connection_id: String,
        message_count: u64,
        duration: Duration,
        bytes_processed: usize,
    ) {
        let avg_latency = if message_count > 0 {
            duration.as_secs_f64() * 1000.0 / message_count as f64
        } else {
            0.0
        };

        // ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°
        if let Ok(mut metrics) = self.current_metrics.lock() {
            metrics.total_requests += message_count;
            metrics.successful_requests += message_count; // ç°¡ç•¥åŒ–
            
            // ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼çµ±è¨ˆæ›´æ–°
            metrics.latency_sum_ms += duration.as_secs_f64() * 1000.0;
            metrics.latency_min_ms = metrics.latency_min_ms.min(avg_latency);
            metrics.latency_max_ms = metrics.latency_max_ms.max(avg_latency);
            
            metrics.last_update = Some(Instant::now());
        }

        // ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆæ›´æ–°
        if let Ok(mut stats) = self.real_time_stats.lock() {
            stats.update_throughput(message_count, duration);
            stats.update_latency(Duration::from_secs_f64(avg_latency / 1000.0));
        }

        // ã‚»ãƒƒã‚·ãƒ§ãƒ³è¿½è·¡æ›´æ–°
        if let Ok(mut sessions) = self.active_sessions.lock() {
            sessions.complete_session(connection_id, message_count, bytes_processed);
        }

        debug!(
            connection_id = %connection_id,
            message_count = message_count,
            duration_ms = duration.as_millis(),
            avg_latency_ms = avg_latency,
            "Stream session completed"
        );
    }

    /// ãƒãƒƒãƒå‡¦ç†ã‚’è¨˜éŒ²
    pub fn record_batch_processing(&self, batch_size: usize, duration: Duration) {
        let throughput = batch_size as f64 / duration.as_secs_f64();
        
        if let Ok(mut stats) = self.real_time_stats.lock() {
            stats.rps_moving_avg.add_sample(throughput);
        }

        debug!(
            batch_size = batch_size,
            duration_ms = duration.as_millis(),
            throughput = throughput,
            "Batch processing recorded"
        );
    }

    /// ãƒãƒƒã‚¯ãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã‚¤ãƒ™ãƒ³ãƒˆã‚’è¨˜éŒ²
    pub fn record_backpressure_event(&self) {
        if let Ok(mut metrics) = self.current_metrics.lock() {
            metrics.backpressure_events += 1;
        }

        warn!("Backpressure event recorded");
    }

    /// ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²
    pub fn record_error(&self, error_type: &str) {
        if let Ok(mut metrics) = self.current_metrics.lock() {
            metrics.failed_requests += 1;
        }

        debug!(error_type = error_type, "Error recorded");
    }

    /// ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ›´æ–°
    pub fn update_memory_usage(&self, current_bytes: usize) {
        if let Ok(mut metrics) = self.current_metrics.lock() {
            metrics.memory_usage_bytes = current_bytes;
            metrics.peak_memory_bytes = metrics.peak_memory_bytes.max(current_bytes);
        }
    }

    /// ãƒ¯ãƒ¼ã‚«ãƒ¼åˆ©ç”¨ç‡ã‚’æ›´æ–°
    pub fn update_worker_utilization(&self, utilization: f64) {
        if let Ok(mut metrics) = self.current_metrics.lock() {
            metrics.worker_utilization = utilization;
        }
    }

    /// ã‚­ãƒ¥ãƒ¼ã®æ·±ã•ã‚’æ›´æ–°
    pub fn update_queue_depth(&self, depth: usize) {
        if let Ok(mut metrics) = self.current_metrics.lock() {
            metrics.queue_depth = depth;
        }
    }

    /// ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã‚’æ›´æ–°
    pub fn update_cache_hit_ratio(&self, hit_ratio: f64) {
        if let Ok(mut metrics) = self.current_metrics.lock() {
            metrics.cache_hit_ratio = hit_ratio;
        }
    }

    /// ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
    pub fn get_current_metrics(&self) -> PerformanceMetrics {
        self.current_metrics.lock()
            .map(|m| m.clone())
            .unwrap_or_default()
    }

    /// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
    pub fn get_performance_summary(&self) -> PerformanceSummary {
        let metrics = self.get_current_metrics();
        let real_time = self.real_time_stats.lock()
            .map(|s| s.get_current_stats())
            .unwrap_or_default();

        PerformanceSummary {
            throughput_rps: real_time.current_rps,
            avg_latency_ms: if metrics.total_requests > 0 {
                metrics.latency_sum_ms / metrics.total_requests as f64
            } else {
                0.0
            },
            p95_latency_ms: metrics.latency_p95_ms,
            p99_latency_ms: metrics.latency_p99_ms,
            success_rate: if metrics.total_requests > 0 {
                metrics.successful_requests as f64 / metrics.total_requests as f64
            } else {
                1.0
            },
            memory_usage_mb: metrics.memory_usage_bytes as f64 / (1024.0 * 1024.0),
            active_connections: metrics.active_connections,
            worker_utilization: metrics.worker_utilization,
            cache_efficiency: metrics.cache_hit_ratio,
            uptime_seconds: metrics.monitoring_start
                .map(|start| start.elapsed().as_secs())
                .unwrap_or(0),
        }
    }

    /// å®šæœŸçš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°ã‚’é–‹å§‹
    fn start_periodic_updates(&self) {
        let current_metrics = Arc::clone(&self.current_metrics);
        let real_time_stats = Arc::clone(&self.real_time_stats);
        let historical_data = Arc::clone(&self.historical_data);
        let interval = self.monitoring_config.collection_interval;

        tokio::spawn(async move {
            let mut ticker = interval(interval);
            
            loop {
                ticker.tick().await;
                
                // ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¨ˆç®—
                if let (Ok(mut metrics), Ok(stats)) = (
                    current_metrics.lock(),
                    real_time_stats.lock()
                ) {
                    let percentiles = stats.calculate_latency_percentiles();
                    metrics.latency_p95_ms = percentiles.0;
                    metrics.latency_p99_ms = percentiles.1;
                    
                    // ç¾åœ¨ã®RPSæ›´æ–°
                    metrics.current_rps = stats.get_current_rps();
                    metrics.peak_rps = metrics.peak_rps.max(metrics.current_rps);
                }

                // å±¥æ­´ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
                if let (Ok(metrics), Ok(mut history)) = (
                    current_metrics.lock(),
                    historical_data.lock()
                ) {
                    let data_point = HistoricalDataPoint {
                        timestamp: Instant::now(),
                        rps: metrics.current_rps,
                        avg_latency_ms: if metrics.total_requests > 0 {
                            metrics.latency_sum_ms / metrics.total_requests as f64
                        } else {
                            0.0
                        },
                        memory_mb: metrics.memory_usage_bytes as f64 / (1024.0 * 1024.0),
                        active_connections: metrics.active_connections,
                        error_rate: if metrics.total_requests > 0 {
                            metrics.failed_requests as f64 / metrics.total_requests as f64
                        } else {
                            0.0
                        },
                    };
                    
                    history.add_data_point(data_point);
                }
            }
        });
    }
}

/// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceSummary {
    pub throughput_rps: f64,
    pub avg_latency_ms: f64,
    pub p95_latency_ms: f64,
    pub p99_latency_ms: f64,
    pub success_rate: f64,           // 0.0-1.0
    pub memory_usage_mb: f64,
    pub active_connections: u64,
    pub worker_utilization: f64,     // 0.0-1.0
    pub cache_efficiency: f64,       // 0.0-1.0
    pub uptime_seconds: u64,
}

impl PerformanceSummary {
    /// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ã¨ã®æ¯”è¼ƒ
    pub fn compare_with_targets(&self, targets: &PerformanceTargets) -> PerformanceComparison {
        PerformanceComparison {
            throughput_achievement: self.throughput_rps / targets.target_rps,
            latency_achievement: targets.target_p95_latency_ms / self.p95_latency_ms,
            memory_efficiency: targets.target_memory_mb / self.memory_usage_mb,
            overall_score: self.calculate_overall_score(targets),
        }
    }

    fn calculate_overall_score(&self, targets: &PerformanceTargets) -> f64 {
        let throughput_score = (self.throughput_rps / targets.target_rps).min(2.0);
        let latency_score = (targets.target_p95_latency_ms / self.p95_latency_ms).min(2.0);
        let memory_score = (targets.target_memory_mb / self.memory_usage_mb).min(2.0);
        let success_score = self.success_rate;

        (throughput_score + latency_score + memory_score + success_score) / 4.0
    }
}

/// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™
#[derive(Debug, Clone)]
pub struct PerformanceTargets {
    pub target_rps: f64,
    pub target_p95_latency_ms: f64,
    pub target_memory_mb: f64,
    pub target_success_rate: f64,
}

impl Default for PerformanceTargets {
    fn default() -> Self {
        Self {
            target_rps: 2000.0,
            target_p95_latency_ms: 25.0,
            target_memory_mb: 100.0, // ä»®ã®å€¤
            target_success_rate: 0.99,
        }
    }
}

/// ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒçµæœ
#[derive(Debug, Clone)]
pub struct PerformanceComparison {
    pub throughput_achievement: f64,  // 1.0 = ç›®æ¨™é”æˆ
    pub latency_achievement: f64,     // 1.0 = ç›®æ¨™é”æˆ  
    pub memory_efficiency: f64,       // 1.0 = ç›®æ¨™é”æˆ
    pub overall_score: f64,           // å…¨ä½“ã‚¹ã‚³ã‚¢
}

// è£œåŠ©çš„ãªæ§‹é€ ä½“ã¨å®Ÿè£…ã¯çœç•¥...
// ï¼ˆMovingAverage, AnomalyDetector, HourlyAggregate, DailyAggregate, SessionStatisticsç­‰ï¼‰
```

## å®Ÿè£…è¨ˆç”»

### Step 1: åŸºæœ¬æ§‹é€ å®Ÿè£… (20åˆ†)
1. StreamPerformanceMonitor ã®åŸºæœ¬æ§‹é€ 
2. PerformanceMetrics ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
3. åŸºæœ¬çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²æ©Ÿèƒ½

### Step 2: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆ (20åˆ†)
1. RealTimeStats å®Ÿè£…
2. ç§»å‹•å¹³å‡ã¨ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¨ˆç®—
3. ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®šæ©Ÿèƒ½

### Step 3: å±¥æ­´ãƒ‡ãƒ¼ã‚¿ç®¡ç† (10åˆ†)
1. HistoricalData ç’°çŠ¶ãƒãƒƒãƒ•ã‚¡å®Ÿè£…
2. ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆè¨˜éŒ²æ©Ÿèƒ½
3. å®šæœŸçš„ãªé›†ç´„å‡¦ç†

### Step 4: ã‚»ãƒƒã‚·ãƒ§ãƒ³è¿½è·¡ã¨çµ±åˆ (10åˆ†)
1. SessionTracker å®Ÿè£…
2. å®šæœŸæ›´æ–°ã‚¿ã‚¹ã‚¯ã®å®Ÿè£…
3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ç”Ÿæˆ

## ãƒ†ã‚¹ãƒˆè¦ä»¶

### åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::time::Duration;

    #[tokio::test]
    async fn test_stream_session_recording() {
        let monitor = StreamPerformanceMonitor::new();
        
        monitor.record_stream_session(
            "test-connection".to_string(),
            100,
            Duration::from_millis(50),
            1024,
        );
        
        let metrics = monitor.get_current_metrics();
        assert_eq!(metrics.total_requests, 100);
        assert!(metrics.latency_sum_ms > 0.0);
    }

    #[tokio::test]
    async fn test_performance_summary() {
        let monitor = StreamPerformanceMonitor::new();
        
        // è¤‡æ•°ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è¨˜éŒ²
        for i in 0..10 {
            monitor.record_stream_session(
                format!("connection-{}", i),
                10,
                Duration::from_millis(20),
                512,
            );
        }

        let summary = monitor.get_performance_summary();
        assert!(summary.throughput_rps >= 0.0);
        assert!(summary.avg_latency_ms >= 0.0);
        assert_eq!(summary.success_rate, 1.0);
    }

    #[test]
    fn test_performance_targets_comparison() {
        let summary = PerformanceSummary {
            throughput_rps: 2500.0,
            p95_latency_ms: 20.0,
            success_rate: 0.995,
            ..Default::default()
        };

        let targets = PerformanceTargets::default();
        let comparison = summary.compare_with_targets(&targets);
        
        assert!(comparison.throughput_achievement > 1.0);
        assert!(comparison.latency_achievement > 1.0);
        assert!(comparison.overall_score > 1.0);
    }
}
```

## æˆåŠŸåŸºæº–

### æ©Ÿèƒ½åŸºæº–
- ã™ã¹ã¦ã®ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒæ­£ç¢ºã«åé›†ã•ã‚Œã‚‹
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆãŒé©åˆ‡ã«æ›´æ–°ã•ã‚Œã‚‹
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ãŒæœŸå¾…é€šã‚Šç”Ÿæˆã•ã‚Œã‚‹
- é•·æ™‚é–“é‹ç”¨ã§ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãŒãªã„

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–
- ç›£è¦–ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ < 2%
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°é…å»¶ < 100ms
- å±¥æ­´ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã®åŠ¹ç‡æ€§

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ å®Œäº†å¾Œï¼š
1. Task 3.7 Fix 07-C: ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ç®¡ç†å®Ÿè£…
2. ä»–ã®æœ€é©åŒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã®ç›£è¦–çµ±åˆ
3. æœ€é©åŒ–åŠ¹æœã®ç¶™ç¶šçš„æ¸¬å®šé–‹å§‹

## é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- Task 3.7 Fix 07-A (åŸºç›¤ã‚¤ãƒ³ãƒ•ãƒ©æ•´å‚™)
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ä»•æ§˜æ›¸
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹